{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2611992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Union\n",
    "import scipy.signal as scisig\n",
    "from scipy.signal import butter, lfilter, freqz\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import (\n",
    "    ReduceLROnPlateau,\n",
    "    OneCycleLR,\n",
    "    CosineAnnealingLR,\n",
    "    CosineAnnealingWarmRestarts,\n",
    ")\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from sklearn.model_selection import GroupKFold,KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d55f13e",
   "metadata": {},
   "source": [
    "# Directory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f56db8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./\"\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce357273",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e05073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Fp1': 0, 'T3': 1, 'C3': 2, 'O1': 3, 'Fp2': 4, 'C4': 5, 'T4': 6, 'O2': 7}\n",
      "['Fp1', 'T3', 'C3', 'O1', 'Fp2', 'C4', 'T4', 'O2']\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    VERSION = '82'\n",
    "\n",
    "    wandb = False\n",
    "    debug = True\n",
    "    create_eegs = False\n",
    "    apex = True\n",
    "    visualize = False\n",
    "    save_all_models = True\n",
    "\n",
    "    if debug:\n",
    "        num_workers = 0\n",
    "        parallel = False\n",
    "    else:\n",
    "        num_workers = os.cpu_count()\n",
    "        parallel = True\n",
    "\n",
    "    model_name = \"resnet1d_gru\"\n",
    "    optimizer = \"AdamW\"\n",
    "\n",
    "    factor = 0.9\n",
    "    eps = 1e-6\n",
    "    lr = 8e-3\n",
    "    min_lr = 1e-6\n",
    "\n",
    "    batch_size = 64\n",
    "    batch_koef_valid = 2\n",
    "    batch_scheduler = True\n",
    "    weight_decay = 1e-2\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1e7\n",
    "\n",
    "    fixed_kernel_size = 5\n",
    "    # N = 9, linear_layer_features = 304\n",
    "    # N = 7, linear_layer_features = 424\n",
    "    # N = 5, linear_layer_features = 880\n",
    "    linear_layer_features = 880   # 1/4, 1/5, 1/6  Signal = 2_000\n",
    "\n",
    "    kernels = [3, 5, 7, 9, 11]\n",
    "    # kernels = [5, 7, 9, 11, 13]\n",
    "\n",
    "    seq_length = 50  # Second's\n",
    "    sampling_rate = 200  # Hz\n",
    "    nsamples = seq_length * sampling_rate\n",
    "    n_split_samples = 5\n",
    "    out_samples = nsamples // n_split_samples  # 2_000\n",
    "    sample_delta = nsamples - out_samples  # 8000\n",
    "    sample_offset = sample_delta // 2\n",
    "    multi_validation = False\n",
    "\n",
    "    train_by_stages = False\n",
    "    train_by_folds = True\n",
    "\n",
    "    # 'GPD', 'GRDA', 'LPD', 'LRDA', 'Other', 'Seizure'\n",
    "    n_stages = 2\n",
    "    train_stages = [0, 1]\n",
    "    epochs = [50, 100]\n",
    "    test_total_eval = 4\n",
    "    total_evals_old = [[(1, 4),(4, 5), (5, 6)], (6, 29)]  # Deprecated\n",
    "    total_evaluators = [ \n",
    "        [   \n",
    "            {'band':(1, 3), 'excl_evals':[]}, \n",
    "            {'band':(4, 4), 'excl_evals':['GPD']}, \n",
    "            {'band':(5, 5), 'excl_evals':[]}, \n",
    "        ], \n",
    "        [   \n",
    "            {'band':(6, 28), 'excl_evals':[]},\n",
    "        ], \n",
    "    ]             \n",
    "    \n",
    "    n_fold = 5\n",
    "    train_folds = [0, 1, 2, 3, 4]\n",
    "    # train_folds = [0]\n",
    "\n",
    "    patience = 11\n",
    "    seed = 2024\n",
    "\n",
    "    bandpass_filter = {\"low\": 0.5, \"high\": 20, \"order\": 2}\n",
    "    rand_filter = {\"probab\": 0.1, \"low\": 10, \"high\": 20, \"band\": 1.0, \"order\": 2}\n",
    "    freq_channels = []  # [(8.0, 12.0)]; [(0.5, 4.5)]\n",
    "    filter_order = 2\n",
    "\n",
    "    random_divide_signal = 0.05\n",
    "    random_close_zone = 0.05\n",
    "    random_common_negative_signal = 0.0\n",
    "    random_common_reverse_signal = 0.0\n",
    "    random_negative_signal = 0.05\n",
    "    random_reverse_signal = 0.05\n",
    "\n",
    "    log_step = 100\n",
    "    log_show = False\n",
    "\n",
    "    scheduler = \"CosineAnnealingWarmRestarts\"  # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts','OneCycleLR']\n",
    "\n",
    "    cosanneal_params = {\n",
    "        \"T_max\": 6,\n",
    "        \"eta_min\": 1e-5,\n",
    "        \"last_epoch\": -1,\n",
    "    }\n",
    "\n",
    "    reduce_params = {\n",
    "        \"mode\": \"min\",\n",
    "        \"factor\": 0.2,\n",
    "        \"patience\": 4,\n",
    "        \"eps\": 1e-6,\n",
    "        \"verbose\": True,\n",
    "    }\n",
    "\n",
    "    cosanneal_res_params = {\n",
    "        \"T_0\": 20,\n",
    "        \"eta_min\": 1e-6,\n",
    "        \"T_mult\": 1,\n",
    "        \"last_epoch\": -1,\n",
    "    }\n",
    "\n",
    "    target_cols = [\n",
    "        \"seizure_vote\",\n",
    "        \"lpd_vote\",\n",
    "        \"gpd_vote\",\n",
    "        \"lrda_vote\",\n",
    "        \"grda_vote\",\n",
    "        \"other_vote\",\n",
    "    ]\n",
    "\n",
    "    pred_cols = [x + \"_pred\" for x in target_cols]\n",
    "\n",
    "    map_features = [\n",
    "        (\"Fp1\", \"T3\"),\n",
    "        (\"T3\", \"O1\"),\n",
    "        (\"Fp1\", \"C3\"),\n",
    "        (\"C3\", \"O1\"),\n",
    "        (\"Fp2\", \"C4\"),\n",
    "        (\"C4\", \"O2\"),\n",
    "        (\"Fp2\", \"T4\"),\n",
    "        (\"T4\", \"O2\"),\n",
    "        #('Fz', 'Cz'), ('Cz', 'Pz'),\n",
    "    ]\n",
    "\n",
    "    eeg_features = [\"Fp1\", \"T3\", \"C3\", \"O1\", \"Fp2\", \"C4\", \"T4\", \"O2\"]  # 'Fz', 'Cz', 'Pz'\n",
    "        # 'F3', 'P3', 'F7', 'T5', 'Fz', 'Cz', 'Pz', 'F4', 'P4', 'F8', 'T6', 'EKG']\n",
    "    feature_to_index = {x: y for x, y in zip(eeg_features, range(len(eeg_features)))}\n",
    "    simple_features = []  # 'Fz', 'Cz', 'Pz', 'EKG'\n",
    "    \n",
    "    n_map_features = len(map_features)\n",
    "    in_channels = n_map_features + n_map_features * len(freq_channels) + len(simple_features)\n",
    "    target_size = len(target_cols)\n",
    "\n",
    "    path_inp = Path(\".\")\n",
    "    file_train = path_inp / \"updated_train.csv\"\n",
    "    path_train = Path('E:/HMS2024/train_eegs')\n",
    "    file_features_test = path_train / \"100261680.parquet\"\n",
    "#     file_eeg_specs = path_inp / \"eeg-spectrogram-by-lead-id-unique/eeg_specs.npy\"\n",
    "    file_raw_eeg = path_inp / \"eegs.npy\"\n",
    "\n",
    "# print(CFG.eeg_feat_size, CFG.in_channels)\n",
    "print(CFG.feature_to_index)\n",
    "print(CFG.eeg_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a98ff534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_logger(log_file=OUTPUT_DIR + \"train.log\"):\n",
    "    from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n",
    "\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def quantize_data(data, classes):\n",
    "    mu_x = mu_law_encoding(data, classes)\n",
    "    return mu_x  # quantized\n",
    "\n",
    "\n",
    "def mu_law_encoding(data, mu):\n",
    "    mu_x = np.sign(data) * np.log(1 + mu * np.abs(data)) / np.log(mu + 1)\n",
    "    return mu_x\n",
    "\n",
    "\n",
    "def mu_law_expansion(data, mu):\n",
    "    s = np.sign(data) * (np.exp(np.abs(data) * np.log(mu + 1)) - 1) / mu\n",
    "    return s\n",
    "\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    return butter(order, [lowcut, highcut], fs=fs, btype=\"band\")\n",
    "\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "def butter_lowpass_filter(\n",
    "    data, cutoff_freq=20, sampling_rate=CFG.sampling_rate, order=4\n",
    "):\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    normal_cutoff = cutoff_freq / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype=\"low\", analog=False)\n",
    "    filtered_data = lfilter(b, a, data, axis=0)\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def denoise_filter(x):\n",
    "    y = butter_bandpass_filter(x, CFG.lowcut, CFG.highcut, CFG.sampling_rate, order=6)\n",
    "    y = (y + np.roll(y, -1) + np.roll(y, -2) + np.roll(y, -3)) / 4\n",
    "    y = y[0:-1:4]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80559e75",
   "metadata": {
    "papermill": {
     "duration": 0.011709,
     "end_time": "2024-02-20T13:40:46.512143",
     "exception": false,
     "start_time": "2024-02-20T13:40:46.500434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Parquet to EEG Signals Numpy Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11f934b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eeg_from_parquet(\n",
    "    parquet_path: str, display: bool = False, seq_length=CFG.seq_length) -> np.ndarray:\n",
    "    eeg = pd.read_parquet(parquet_path, columns=CFG.eeg_features)\n",
    "    rows = len(eeg)\n",
    "\n",
    "    offset = (rows - CFG.nsamples) // 2\n",
    "\n",
    "    eeg = eeg.iloc[offset : offset + CFG.nsamples]\n",
    "\n",
    "    if display:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        offset = 0\n",
    "\n",
    "    data = np.zeros((CFG.nsamples, len(CFG.eeg_features)))\n",
    "\n",
    "    for index, feature in enumerate(CFG.eeg_features):\n",
    "        x = eeg[feature].values.astype(\"float32\") \n",
    "        mean = np.nanmean(x)\n",
    "        nan_percentage = np.isnan(x).mean()\n",
    "        if nan_percentage < 1: \n",
    "            x = np.nan_to_num(x, nan=mean)\n",
    "        else:  \n",
    "            x[:] = 0\n",
    "        data[:, index] = x\n",
    "        if display:\n",
    "            if index != 0:\n",
    "                offset += x.max()\n",
    "            plt.plot(range(CFG.nsamples), x - offset, label=feature)\n",
    "            offset -= x.min()\n",
    "\n",
    "    if display:\n",
    "        plt.legend()\n",
    "        name = parquet_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        plt.yticks([])\n",
    "        plt.title(f\"EEG {name}\", size=16)\n",
    "        plt.show()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e92dcd3",
   "metadata": {
    "papermill": {
     "duration": 0.014802,
     "end_time": "2024-02-20T13:42:17.914507",
     "exception": false,
     "start_time": "2024-02-20T13:42:17.899705",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f7d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        batch_size: int,\n",
    "        eegs: Dict[int, np.ndarray],\n",
    "        mode: str = \"train\",\n",
    "        downsample: int = None,\n",
    "        bandpass_filter: Dict[str, Union[int, float]] = None,\n",
    "        rand_filter: Dict[str, Union[int, float]] = None,\n",
    "    ):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        self.eegs = eegs\n",
    "        self.downsample = downsample\n",
    "        self.offset = None\n",
    "        self.bandpass_filter = bandpass_filter\n",
    "        self.rand_filter = rand_filter\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X, y_prob = self.__data_generation(index)\n",
    "        if self.downsample is not None:\n",
    "            X = X[:: self.downsample, :]\n",
    "        output = {\n",
    "            \"eeg\": torch.tensor(X, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(y_prob, dtype=torch.float32),\n",
    "        }\n",
    "        return output\n",
    "\n",
    "    def set_offset(self, offset: int):\n",
    "        self.offset = offset\n",
    "\n",
    "    def __data_generation(self, index):\n",
    "        X = np.zeros(\n",
    "            (CFG.out_samples, CFG.in_channels), dtype=\"float32\"\n",
    "        )  # Size=(10000, 14)\n",
    "\n",
    "        random_divide_signal = False\n",
    "        row = self.df.iloc[index]  # Строка Pandas\n",
    "        data = self.eegs[row.eeg_id]  # Size=(10000, 8)\n",
    "        if CFG.nsamples != CFG.out_samples:\n",
    "            if self.mode == \"train\":\n",
    "                offset = (CFG.sample_delta * random.randint(0, 1000)) // 1000\n",
    "            elif not self.offset is None:\n",
    "                offset = self.offset\n",
    "            else:\n",
    "                offset = CFG.sample_offset\n",
    "\n",
    "            if self.mode == \"train\" and CFG.random_divide_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_divide_signal:\n",
    "                random_divide_signal = True\n",
    "                multipliers = [(1, 2), (2, 3), (3, 4), (3, 5)]\n",
    "                koef_1, koef_2 = multipliers[random.randint(0, 3)]\n",
    "                offset = (koef_1 * offset) // koef_2\n",
    "                data = data[offset:offset+(CFG.out_samples * koef_2) // koef_1,:]\n",
    "            else:\n",
    "                data = data[offset:offset+CFG.out_samples,:]\n",
    "\n",
    "        reverse_signal = False\n",
    "        negative_signal = False\n",
    "        if self.mode == \"train\":\n",
    "            if CFG.random_common_reverse_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_common_reverse_signal:\n",
    "                reverse_signal = True\n",
    "            if CFG.random_common_negative_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_common_negative_signal:\n",
    "                negative_signal = True\n",
    "\n",
    "        for i, (feat_a, feat_b) in enumerate(CFG.map_features):\n",
    "            if self.mode == \"train\" and CFG.random_close_zone > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_close_zone:\n",
    "                continue\n",
    "            \n",
    "            diff_feat = (\n",
    "                data[:, CFG.feature_to_index[feat_a]]\n",
    "                - data[:, CFG.feature_to_index[feat_b]]\n",
    "            )  # Size=(10000,)\n",
    "\n",
    "            if self.mode == \"train\":\n",
    "                if reverse_signal or CFG.random_reverse_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_reverse_signal:\n",
    "                    diff_feat = np.flip(diff_feat)\n",
    "                if negative_signal or CFG.random_negative_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_negative_signal:\n",
    "                    diff_feat = -diff_feat\n",
    "\n",
    "            if not self.bandpass_filter is None:\n",
    "                diff_feat = butter_bandpass_filter(\n",
    "                    diff_feat,\n",
    "                    self.bandpass_filter[\"low\"],\n",
    "                    self.bandpass_filter[\"high\"],\n",
    "                    CFG.sampling_rate,\n",
    "                    order=self.bandpass_filter[\"order\"],\n",
    "                )\n",
    "            \n",
    "            if random_divide_signal:\n",
    "                #diff_feat = cp.asnumpy(cpsig.upfirdn([1.0, 1, 1.0], diff_feat, 2, 3))  # linear interp, rate 2/3\n",
    "                diff_feat = scisig.upfirdn([1.0, 1, 1.0], diff_feat, koef_1, koef_2)  # linear interp, rate 2/3\n",
    "                diff_feat = diff_feat[0:CFG.out_samples]\n",
    "\n",
    "            if (\n",
    "                self.mode == \"train\"\n",
    "                and not self.rand_filter is None\n",
    "                and random.uniform(0.0, 1.0) <= self.rand_filter[\"probab\"]\n",
    "            ):\n",
    "                lowcut = random.randint(\n",
    "                    self.rand_filter[\"low\"], self.rand_filter[\"high\"]\n",
    "                )\n",
    "                highcut = lowcut + self.rand_filter[\"band\"]\n",
    "                diff_feat = butter_bandpass_filter(\n",
    "                    diff_feat,\n",
    "                    lowcut,\n",
    "                    highcut,\n",
    "                    CFG.sampling_rate,\n",
    "                    order=self.rand_filter[\"order\"],\n",
    "                )\n",
    "\n",
    "            X[:, i] = diff_feat\n",
    "\n",
    "        n = CFG.n_map_features\n",
    "        if len(CFG.freq_channels) > 0:\n",
    "            for i in range(CFG.n_map_features):\n",
    "                diff_feat = X[:, i]\n",
    "                for j, (lowcut, highcut) in enumerate(CFG.freq_channels):\n",
    "                    band_feat = butter_bandpass_filter(\n",
    "                        diff_feat, lowcut, highcut, CFG.sampling_rate, order=CFG.filter_order,  # 6\n",
    "                    )\n",
    "                    X[:, n] = band_feat\n",
    "                    n += 1\n",
    "\n",
    "        for spml_feat in CFG.simple_features:\n",
    "            feat_val = data[:, CFG.feature_to_index[spml_feat]]\n",
    "            \n",
    "            if not self.bandpass_filter is None:\n",
    "                feat_val = butter_bandpass_filter(\n",
    "                    feat_val,\n",
    "                    self.bandpass_filter[\"low\"],\n",
    "                    self.bandpass_filter[\"high\"],\n",
    "                    CFG.sampling_rate,\n",
    "                    order=self.bandpass_filter[\"order\"],\n",
    "                )\n",
    "\n",
    "            if (\n",
    "                self.mode == \"train\"\n",
    "                and not self.rand_filter is None\n",
    "                and random.uniform(0.0, 1.0) <= self.rand_filter[\"probab\"]\n",
    "            ):\n",
    "                lowcut = random.randint(\n",
    "                    self.rand_filter[\"low\"], self.rand_filter[\"high\"]\n",
    "                )\n",
    "                highcut = lowcut + self.rand_filter[\"band\"]\n",
    "                feat_val = butter_bandpass_filter(\n",
    "                    feat_val,\n",
    "                    lowcut,\n",
    "                    highcut,\n",
    "                    CFG.sampling_rate,\n",
    "                    order=self.rand_filter[\"order\"],\n",
    "                )\n",
    "\n",
    "            X[:, n] = feat_val\n",
    "            n += 1\n",
    "            \n",
    "        X = np.clip(X, -1024, 1024)\n",
    "\n",
    "        X = np.nan_to_num(X, nan=0) / 32.0\n",
    "\n",
    "        X = butter_lowpass_filter(X, order=CFG.filter_order)  # 4\n",
    "\n",
    "        y_prob = np.zeros(CFG.target_size, dtype=\"float32\")  # Size=(6,)\n",
    "        if self.mode != \"test\":\n",
    "            y_prob = row[CFG.target_cols].values.astype(np.float32)\n",
    "\n",
    "        return X, y_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6d124c",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af20dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLDivLossWithLogits(nn.KLDivLoss):\n",
    "    def __init__(self):\n",
    "        super().__init__(reduction=\"batchmean\")\n",
    "\n",
    "    def forward(self, y, t):\n",
    "        y = nn.functional.log_softmax(y, dim=1)\n",
    "        loss = super().forward(y, t)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556d75c",
   "metadata": {
    "papermill": {
     "duration": 0.031018,
     "end_time": "2024-02-20T13:42:28.757460",
     "exception": false,
     "start_time": "2024-02-20T13:42:28.726442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24ef8225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_1D_Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        downsampling,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super(ResNet_1D_Block, self).__init__()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=in_channels)\n",
    "        self.relu_1 = nn.Hardswish()\n",
    "        self.relu_2 = nn.Hardswish()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout, inplace=False)\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=out_channels)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.maxpool = nn.MaxPool1d(\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "            padding=0,\n",
    "            dilation=dilation,\n",
    "        )\n",
    "        self.downsampling = downsampling\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu_1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu_2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out = self.maxpool(out)\n",
    "        identity = self.downsampling(x)\n",
    "\n",
    "        out += identity\n",
    "        return out\n",
    "\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernels,\n",
    "        in_channels,\n",
    "        fixed_kernel_size,\n",
    "        num_classes,\n",
    "        linear_layer_features,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "    ):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.kernels = kernels\n",
    "        self.planes = 24\n",
    "        self.parallel_conv = nn.ModuleList()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        for i, kernel_size in enumerate(list(self.kernels)):\n",
    "            sep_conv = nn.Conv1d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=self.planes,\n",
    "                kernel_size=(kernel_size),\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                dilation=dilation,\n",
    "                groups=groups,\n",
    "                bias=False,\n",
    "            )\n",
    "            self.parallel_conv.append(sep_conv)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=self.planes)\n",
    "        self.relu_1 = nn.SiLU()\n",
    "        self.relu_2 = nn.SiLU()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=self.planes,\n",
    "            out_channels=self.planes,\n",
    "            kernel_size=fixed_kernel_size,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.block = self._make_resnet_layer(\n",
    "            kernel_size=fixed_kernel_size,\n",
    "            stride=1,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            padding=fixed_kernel_size // 2,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=self.planes)\n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=6, stride=6, padding=2)\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.in_channels,\n",
    "            hidden_size=128,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            # dropout=0.2,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(in_features=linear_layer_features, out_features=num_classes)\n",
    "\n",
    "    def _make_resnet_layer(\n",
    "        self,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        blocks=5,\n",
    "        padding=0,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        layers = []\n",
    "\n",
    "        for i in range(blocks):\n",
    "            downsampling = nn.Sequential(\n",
    "                nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "            )\n",
    "            layers.append(\n",
    "                ResNet_1D_Block(\n",
    "                    in_channels=self.planes,\n",
    "                    out_channels=self.planes,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    padding=padding,\n",
    "                    downsampling=downsampling,\n",
    "                    dilation=dilation,\n",
    "                    groups=groups,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "            )\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        out_sep = []\n",
    "        for i in range(len(self.kernels)):\n",
    "            sep = self.parallel_conv[i](x)\n",
    "            out_sep.append(sep)\n",
    "\n",
    "        out = torch.cat(out_sep, dim=2)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu_1(out)\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        out = self.block(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu_2(out)\n",
    "        out = self.avgpool(out)\n",
    "\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        rnn_out, _ = self.rnn(x.permute(0, 2, 1))\n",
    "        new_rnn_h = rnn_out[:, -1, :]\n",
    "\n",
    "        new_out = torch.cat([out, new_rnn_h], dim=1)\n",
    "        return new_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_out = self.extract_features(x)\n",
    "        result = self.fc(new_out)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a8d764",
   "metadata": {},
   "source": [
    "# Adan Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a7d6d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adan(Optimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=1e-3,\n",
    "        betas=(0.98, 0.92, 0.99),\n",
    "        eps=1e-8,\n",
    "        weight_decay=0.2,\n",
    "        max_grad_norm=0.0,\n",
    "        no_prox=False,\n",
    "    ):\n",
    "        if not 0.0 <= max_grad_norm:\n",
    "            raise ValueError(\"Invalid Max grad norm: {}\".format(max_grad_norm))\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        if not 0.0 <= betas[2] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 2: {}\".format(betas[2]))\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            eps=eps,\n",
    "            weight_decay=weight_decay,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            no_prox=no_prox,\n",
    "        )\n",
    "        super(Adan, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adan, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault(\"no_prox\", False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def restart_opt(self):\n",
    "        for group in self.param_groups:\n",
    "            group[\"step\"] = 0\n",
    "            for p in group[\"params\"]:\n",
    "                if p.requires_grad:\n",
    "                    state = self.state[p]\n",
    "                    # State initialization\n",
    "\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of gradient difference\n",
    "                    state[\"exp_avg_diff\"] = torch.zeros_like(p)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        if self.defaults[\"max_grad_norm\"] > 0:\n",
    "            device = self.param_groups[0][\"params\"][0].device\n",
    "            global_grad_norm = torch.zeros(1, device=device)\n",
    "\n",
    "            max_grad_norm = torch.tensor(self.defaults[\"max_grad_norm\"], device=device)\n",
    "            for group in self.param_groups:\n",
    "\n",
    "                for p in group[\"params\"]:\n",
    "                    if p.grad is not None:\n",
    "                        grad = p.grad\n",
    "                        global_grad_norm.add_(grad.pow(2).sum())\n",
    "\n",
    "            global_grad_norm = torch.sqrt(global_grad_norm)\n",
    "\n",
    "            clip_global_grad_norm = torch.clamp(\n",
    "                max_grad_norm / (global_grad_norm + group[\"eps\"]), max=1.0\n",
    "            )\n",
    "        else:\n",
    "            clip_global_grad_norm = 1.0\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            beta1, beta2, beta3 = group[\"betas\"]\n",
    "            # assume same step across group now to simplify things\n",
    "            # per parameter step can be easily support by making it tensor, or pass list into kernel\n",
    "            if \"step\" in group:\n",
    "                group[\"step\"] += 1\n",
    "            else:\n",
    "                group[\"step\"] = 1\n",
    "\n",
    "            bias_correction1 = 1.0 - beta1 ** group[\"step\"]\n",
    "            bias_correction2 = 1.0 - beta2 ** group[\"step\"]\n",
    "            bias_correction3 = 1.0 - beta3 ** group[\"step\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p)\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p)\n",
    "                    state[\"exp_avg_diff\"] = torch.zeros_like(p)\n",
    "\n",
    "                grad = p.grad.mul_(clip_global_grad_norm)\n",
    "                if \"pre_grad\" not in state or group[\"step\"] == 1:\n",
    "                    state[\"pre_grad\"] = grad\n",
    "\n",
    "                copy_grad = grad.clone()\n",
    "\n",
    "                exp_avg, exp_avg_sq, exp_avg_diff = (\n",
    "                    state[\"exp_avg\"],\n",
    "                    state[\"exp_avg_sq\"],\n",
    "                    state[\"exp_avg_diff\"],\n",
    "                )\n",
    "                diff = grad - state[\"pre_grad\"]\n",
    "\n",
    "                update = grad + beta2 * diff\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  # m_t\n",
    "                exp_avg_diff.mul_(beta2).add_(diff, alpha=1 - beta2)  # diff_t\n",
    "                exp_avg_sq.mul_(beta3).addcmul_(update, update, value=1 - beta3)  # n_t\n",
    "\n",
    "                denom = ((exp_avg_sq).sqrt() / math.sqrt(bias_correction3)).add_(\n",
    "                    group[\"eps\"]\n",
    "                )\n",
    "                update = (\n",
    "                    (\n",
    "                        exp_avg / bias_correction1\n",
    "                        + beta2 * exp_avg_diff / bias_correction2\n",
    "                    )\n",
    "                ).div_(denom)\n",
    "\n",
    "                if group[\"no_prox\"]:\n",
    "                    p.data.mul_(1 - group[\"lr\"] * group[\"weight_decay\"])\n",
    "                    p.add_(update, alpha=-group[\"lr\"])\n",
    "                else:\n",
    "                    p.add_(update, alpha=-group[\"lr\"])\n",
    "                    p.data.div_(1 + group[\"lr\"] * group[\"weight_decay\"])\n",
    "\n",
    "                state[\"pre_grad\"] = copy_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9005e982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150246"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EEGNet(\n",
    "        kernels=CFG.kernels,\n",
    "        in_channels=CFG.in_channels,\n",
    "        fixed_kernel_size=CFG.fixed_kernel_size,\n",
    "        num_classes=CFG.target_size,\n",
    "        linear_layer_features=CFG.linear_layer_features,\n",
    "    )\n",
    "num_parameters = sum(p.numel() for p in model.parameters())\n",
    "num_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5986566",
   "metadata": {},
   "source": [
    "# Train func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "622880db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    stage, fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        eegs = batch[\"eeg\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            y_preds = model(eegs)\n",
    "            loss = criterion(F.log_softmax(y_preds, dim=1), labels)\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), CFG.max_grad_norm\n",
    "        )\n",
    "\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "\n",
    "        if CFG.log_show and (\n",
    "            step % CFG.log_step == 0 or step == (len(train_loader) - 1)\n",
    "        ):\n",
    "            # remain=timeSince(start, float(step + 1) / len(train_loader))\n",
    "            LOGGER.info(\n",
    "                f\"Epoch {epoch+1} [{step}/{len(train_loader)}] Loss: {losses.val:.4f} Loss Avg:{losses.avg:.4f}\"\n",
    "            )\n",
    "            # \"Elapsed {remain:s} Grad: {grad_norm:.4f}  LR: {cheduler.get_lr()[0]:.8f}\"\n",
    "\n",
    "        if CFG.wandb:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    f\"[fold{fold}] loss\": losses.val,\n",
    "                    f\"[fold{fold}] lr\": scheduler.get_lr()[0],\n",
    "                }\n",
    "            )\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6474a",
   "metadata": {},
   "source": [
    "# Valid Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c417fc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(stage, epoch, valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "    start = end = time.time()\n",
    "\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        eegs = batch[\"eeg\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(eegs)\n",
    "            loss = criterion(F.log_softmax(y_preds, dim=1), labels)\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(nn.Softmax(dim=1)(y_preds).to(\"cpu\").numpy())\n",
    "        targets.append(labels.to(\"cpu\").numpy())\n",
    "        end = time.time()\n",
    "\n",
    "        if CFG.log_show and (\n",
    "            step % CFG.log_step == 0 or step == (len(valid_loader) - 1)\n",
    "        ):\n",
    "            # remain=timeSince(start, float(step + 1) / len(valid_loader))\n",
    "            LOGGER.info(\n",
    "                f\"Epoch {epoch+1} VALIDATION: [{step}/{len(valid_loader)}] Val Loss: {losses.val:.4f} Val Loss Avg: {losses.avg:.4f}\"\n",
    "            )\n",
    "            # Elapsed {remain:s}\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    targets = np.concatenate(targets)\n",
    "\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af84f21",
   "metadata": {
    "papermill": {
     "duration": 0.031254,
     "end_time": "2024-02-20T13:42:29.808216",
     "exception": false,
     "start_time": "2024-02-20T13:42:29.776962",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Build Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96f27f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(cfg, model, device, epochs, num_batches_per_epoch):\n",
    "    lr = cfg.lr\n",
    "    # lr = default_configs[\"lr\"]\n",
    "    if cfg.optimizer == \"SAM\":\n",
    "        base_optimizer = (\n",
    "            torch.optim.SGD\n",
    "        )  # define an optimizer for the \"sharpness-aware\" update\n",
    "        optimizer_model = SAM(\n",
    "            model.parameters(),\n",
    "            base_optimizer,\n",
    "            lr=lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=cfg.weight_decay,\n",
    "            adaptive=True,\n",
    "        )\n",
    "    elif cfg.optimizer == \"Ranger21\":\n",
    "        optimizer_model = Ranger21(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=cfg.weight_decay,\n",
    "            num_epochs=epochs,\n",
    "            num_batches_per_epoch=num_batches_per_epoch,\n",
    "        )\n",
    "    elif cfg.optimizer == \"SGD\":\n",
    "        optimizer_model = torch.optim.SGD(\n",
    "            model.parameters(), lr=lr, weight_decay=cfg.weight_decay, momentum=0.9\n",
    "        )\n",
    "    elif cfg.optimizer == \"Adam\":\n",
    "        optimizer_model = Adam(model.parameters(), lr=lr, weight_decay=CFG.weight_decay)\n",
    "    elif cfg.optimizer == \"AdamW\":\n",
    "        optimizer_model = AdamW(\n",
    "            model.parameters(), lr=lr, weight_decay=CFG.weight_decay\n",
    "        )\n",
    "    elif cfg.optimizer == \"Lion\":\n",
    "        optimizer_model = Lion(model.parameters(), lr=lr, weight_decay=cfg.weight_decay)\n",
    "    elif cfg.optimizer == \"Adan\":\n",
    "        optimizer_model = Adan(model.parameters(), lr=lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    return optimizer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c88c6",
   "metadata": {},
   "source": [
    "# Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "394570ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, epochs, steps_per_epoch):\n",
    "    if CFG.scheduler == \"ReduceLROnPlateau\":\n",
    "        scheduler = ReduceLROnPlateau(optimizer, **CFG.reduce_params)\n",
    "    elif CFG.scheduler == \"CosineAnnealingLR\":\n",
    "        scheduler = CosineAnnealingLR(optimizer, **CFG.cosanneal_params)\n",
    "    elif CFG.scheduler == \"CosineAnnealingWarmRestarts\":\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, **CFG.cosanneal_res_params)\n",
    "    elif CFG.scheduler == \"OneCycleLR\":\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer=optimizer,\n",
    "            epochs=epochs,\n",
    "            pct_start=0.0,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            max_lr=CFG.lr,\n",
    "            div_factor=25,\n",
    "            final_div_factor=4.0e-01,\n",
    "        )\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8dcbbf",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3524d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(stage, epochs, folds, fold, directory, prev_dir, eggs):\n",
    "    train_folds = folds[folds[\"fold\"] != fold].reset_index(drop=True)\n",
    "    valid_folds = folds[folds[\"fold\"] == fold].reset_index(drop=True)\n",
    "    valid_labels = valid_folds[CFG.target_cols].values\n",
    "\n",
    "    train_dataset = EEGDataset(\n",
    "        train_folds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        mode=\"train\",\n",
    "        eegs=eggs,\n",
    "        bandpass_filter=CFG.bandpass_filter,\n",
    "        rand_filter=CFG.rand_filter,\n",
    "    )\n",
    "        \n",
    "    valid_dataset = EEGDataset(\n",
    "        valid_folds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        mode=\"valid\",\n",
    "        eegs=eggs,\n",
    "        bandpass_filter=CFG.bandpass_filter,\n",
    "        #rand_filter=CFG.rand_filter,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=CFG.batch_size * CFG.batch_koef_valid,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    LOGGER.info(\n",
    "        f\"========== stage: {stage} fold: {fold} training {len(train_loader)} / {len(valid_loader)} ==========\"\n",
    "    )\n",
    "\n",
    "    model = EEGNet(\n",
    "        kernels=CFG.kernels,\n",
    "        in_channels=CFG.in_channels,\n",
    "        fixed_kernel_size=CFG.fixed_kernel_size,\n",
    "        num_classes=CFG.target_size,\n",
    "        linear_layer_features=CFG.linear_layer_features,\n",
    "    )\n",
    "\n",
    "    if stage > 1:\n",
    "        model_weight = f\"{prev_dir}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage-1}_fold-{fold}_best.pth\"\n",
    "        checkpoint = torch.load(model_weight, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # CPMP: wrap the model to use all GPUs\n",
    "    if CFG.parallel:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    optimizer = build_optimizer(\n",
    "        CFG, model, device, epochs=epochs, num_batches_per_epoch=len(train_loader)\n",
    "    )\n",
    "    scheduler = get_scheduler(\n",
    "        optimizer, epochs=epochs, steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "    best_score = np.inf\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(\n",
    "            stage,\n",
    "            fold,\n",
    "            train_loader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            scheduler,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        # eval\n",
    "        valid_dataset.set_offset(CFG.sample_offset)\n",
    "        avg_val_loss, predictions = valid_fn(\n",
    "            stage,\n",
    "            epoch,\n",
    "            valid_loader,\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "        )\n",
    "        \n",
    "        avg_loss_line = ''\n",
    "        if CFG.multi_validation:\n",
    "            multi_avg_val_loss = np.zeros(CFG.n_split_samples)\n",
    "            start = (2 * CFG.sample_delta) // CFG.n_split_samples\n",
    "            finish = (3 * CFG.sample_delta) // CFG.n_split_samples\n",
    "            delta = (finish - start) // 5\n",
    "            for i in range(CFG.n_split_samples):\n",
    "                valid_dataset.set_offset(start)\n",
    "                multi_avg_val_loss[i], _ = valid_fn(\n",
    "                    stage,\n",
    "                    epoch,\n",
    "                    valid_loader,\n",
    "                    model,\n",
    "                    criterion,\n",
    "                    device,\n",
    "                )\n",
    "                avg_loss_line += f\" {multi_avg_val_loss[i]:.4f}\"\n",
    "                start += delta\n",
    "            avg_loss_line += f\" mean={np.mean(multi_avg_val_loss):.4f}\"\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(\n",
    "            f\"Epoch {epoch+1} Avg Train Loss: {avg_loss:.4f} Avg Valid Loss: {avg_val_loss:.4f} / {avg_loss_line}\"\n",
    "        )\n",
    "        #   time: {elapsed:.0f}s\n",
    "        if CFG.wandb:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    f\"[fold{fold}] stage\": stage,\n",
    "                    f\"[fold{fold}] epoch\": epoch + 1,\n",
    "                    f\"[fold{fold}] avg_train_loss\": avg_loss,\n",
    "                    f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                    #f\"[fold{fold}] score\": score,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if CFG.save_all_models:\n",
    "            torch.save(\n",
    "                {\"model\": model.state_dict(), \"predictions\": predictions},\n",
    "                f\"{directory}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage}_fold-{fold}_epoch-{epoch}_val-{avg_val_loss:.4f}_train-{avg_loss:.4f}.pth\",\n",
    "            )\n",
    "\n",
    "        if best_score > avg_val_loss:\n",
    "            best_score = avg_val_loss\n",
    "            LOGGER.info(f\"Epoch {epoch+1} Save Best Valid Loss: {avg_val_loss:.4f}\")\n",
    "            # CPMP: save the original model. It is stored as the module attribute of the DP model.\n",
    "            torch.save(\n",
    "                {\"model\": model.state_dict(), \"predictions\": predictions},\n",
    "                f\"{directory}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage}_fold-{fold}_best.pth\",\n",
    "            )\n",
    "\n",
    "    predictions = torch.load(\n",
    "        f\"{directory}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage}_fold-{fold}_best.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )[\"predictions\"]\n",
    "\n",
    "    # valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n",
    "    valid_folds[CFG.pred_cols] = predictions\n",
    "    valid_folds[CFG.target_cols] = valid_labels\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return valid_folds, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715bd694",
   "metadata": {},
   "source": [
    "# Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d33e0153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (100784, 15)\n",
      "Targets ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
      "There are 1921 patients in the training data.\n",
      "There are 16064 EEG IDs in the training data.\n",
      "There are 18996 unique eeg_id + votes in the training data.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(CFG.file_train)\n",
    "TARGETS = train.columns[-6:]\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Targets\", list(TARGETS))\n",
    "\n",
    "train[\"total_evaluators\"] = train[CFG.target_cols].sum(axis=1)\n",
    "\n",
    "train_uniq = train.drop_duplicates(subset=[\"eeg_id\"] + list(TARGETS))\n",
    "\n",
    "print(f\"There are {train.patient_id.nunique()} patients in the training data.\")\n",
    "print(f\"There are {train.eeg_id.nunique()} EEG IDs in the training data.\")\n",
    "print(f\"There are {train_uniq.shape[0]} unique eeg_id + votes in the training data.\")\n",
    "\n",
    "if CFG.visualize:\n",
    "    train_uniq.eeg_id.value_counts().value_counts().plot(\n",
    "        kind=\"bar\",\n",
    "        title=f\"Distribution of Count of EEG w Unique Vote: \"\n",
    "        f\"{train_uniq.shape[0]} examples\",\n",
    "    )\n",
    "\n",
    "del train_uniq\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "248b3843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20 raw eeg features\n",
      "['Fp1', 'F3', 'C3', 'P3', 'F7', 'T3', 'T5', 'O1', 'Fz', 'Cz', 'Pz', 'Fp2', 'F4', 'C4', 'P4', 'F8', 'T4', 'T6', 'O2', 'EKG']\n"
     ]
    }
   ],
   "source": [
    "if CFG.visualize:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(train[\"total_evaluators\"], bins=10, color=\"blue\", edgecolor=\"black\")\n",
    "    plt.title(\"Histogram of Total Evaluators\")\n",
    "    plt.xlabel(\"Total Evaluators\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "tst_eeg_df = pd.read_parquet(CFG.file_features_test)\n",
    "tst_eeg_features = tst_eeg_df.columns\n",
    "print(f\"There are {len(tst_eeg_features)} raw eeg features\")\n",
    "print(list(tst_eeg_features))\n",
    "del tst_eeg_df\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff051768",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c83f1ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[127492639,\n",
       " 1978807404,\n",
       " 557980729,\n",
       " 4099147263,\n",
       " 1963161945,\n",
       " 3631726128,\n",
       " 4265493714,\n",
       " 3051612434,\n",
       " 3774460362,\n",
       " 2678832786,\n",
       " 2922645408,\n",
       " 212995060,\n",
       " 2581514982,\n",
       " 92643023,\n",
       " 116172961,\n",
       " 643458890,\n",
       " 2564153803,\n",
       " 239963779,\n",
       " 1605170669,\n",
       " 474823595,\n",
       " 3042959589,\n",
       " 1672004489,\n",
       " 3001079370,\n",
       " 1554550678,\n",
       " 3616273655,\n",
       " 2692343476,\n",
       " 164296540,\n",
       " 4133262183,\n",
       " 1900116647,\n",
       " 1418039992,\n",
       " 3424058528,\n",
       " 3368113098,\n",
       " 2825559136,\n",
       " 2660894080,\n",
       " 2768248759,\n",
       " 4044160416,\n",
       " 1456770042,\n",
       " 2214815956,\n",
       " 1832101066,\n",
       " 2192283673,\n",
       " 1622817309,\n",
       " 462363590,\n",
       " 4233854567,\n",
       " 1575215077,\n",
       " 3782281219,\n",
       " 4226410738,\n",
       " 807031283,\n",
       " 2944613501,\n",
       " 2100670995,\n",
       " 2402782492,\n",
       " 361209024,\n",
       " 513641250,\n",
       " 4117135159,\n",
       " 135897239,\n",
       " 4193531390,\n",
       " 2075631178,\n",
       " 2458607184,\n",
       " 245081975,\n",
       " 1827123696,\n",
       " 3074802187,\n",
       " 814868672,\n",
       " 1158561579,\n",
       " 3191986017,\n",
       " 814407858,\n",
       " 3587168508,\n",
       " 2926135348,\n",
       " 2971910547,\n",
       " 1943001855,\n",
       " 914021164,\n",
       " 296816339,\n",
       " 3812258567,\n",
       " 4070077333,\n",
       " 213732050,\n",
       " 772789399,\n",
       " 790125239,\n",
       " 3974368366,\n",
       " 1153628412,\n",
       " 3488615355,\n",
       " 4279854660,\n",
       " 838438889,\n",
       " 1988969709,\n",
       " 1443603429,\n",
       " 3525243172,\n",
       " 2075155345,\n",
       " 640352706,\n",
       " 1093801927,\n",
       " 3810763641,\n",
       " 3914505153,\n",
       " 3600498265,\n",
       " 3593590471,\n",
       " 1728591652,\n",
       " 579420136,\n",
       " 316332068,\n",
       " 4137598087,\n",
       " 1420296251,\n",
       " 561711604,\n",
       " 3236235107,\n",
       " 3324808409,\n",
       " 837269560,\n",
       " 3357921174,\n",
       " 3676006170,\n",
       " 4042379680,\n",
       " 3709630489,\n",
       " 3849716072,\n",
       " 1689400488,\n",
       " 89934411,\n",
       " 2324916116,\n",
       " 814647849,\n",
       " 2387222601,\n",
       " 1482143177,\n",
       " 3814859698,\n",
       " 753949645,\n",
       " 3499039900,\n",
       " 2978688883,\n",
       " 918067029,\n",
       " 81796855,\n",
       " 799074690,\n",
       " 852171773,\n",
       " 943497631,\n",
       " 4044280036,\n",
       " 431853514,\n",
       " 817339512,\n",
       " 3301866839,\n",
       " 924644919,\n",
       " 1676430645,\n",
       " 3075923156,\n",
       " 2088905988,\n",
       " 3254468733,\n",
       " 2592113940,\n",
       " 441218673,\n",
       " 2841962361,\n",
       " 2297156197,\n",
       " 988102591,\n",
       " 3194302792,\n",
       " 3147505992,\n",
       " 974316723,\n",
       " 589521334,\n",
       " 2032696205,\n",
       " 2676749133,\n",
       " 3961661389,\n",
       " 3625283682,\n",
       " 1513873828,\n",
       " 3727264051,\n",
       " 137197229,\n",
       " 1870518557,\n",
       " 3589422612,\n",
       " 3253961227,\n",
       " 990882124,\n",
       " 1141688300,\n",
       " 19267257,\n",
       " 1033765668,\n",
       " 2589579883,\n",
       " 3518820182,\n",
       " 1055142000,\n",
       " 2715080100,\n",
       " 986610964,\n",
       " 3982868315,\n",
       " 269317153,\n",
       " 1356603492,\n",
       " 4046089756,\n",
       " 756540942,\n",
       " 4125944569,\n",
       " 2734993945,\n",
       " 1791384586,\n",
       " 1436583046,\n",
       " 1986751753,\n",
       " 4229174525,\n",
       " 1648238813,\n",
       " 1305119316,\n",
       " 1886794071,\n",
       " 2784870122,\n",
       " 3992276805,\n",
       " 3997989413,\n",
       " 805726904,\n",
       " 1619682726,\n",
       " 2664500024,\n",
       " 814506987,\n",
       " 3853127128,\n",
       " 3674325432,\n",
       " 2404091395,\n",
       " 2731569145,\n",
       " 629665312,\n",
       " 2075616118,\n",
       " 2200631965,\n",
       " 746000760,\n",
       " 2333157858,\n",
       " 4090930465,\n",
       " 2472152800,\n",
       " 1237517128,\n",
       " 1315133082,\n",
       " 407431860,\n",
       " 1343172305,\n",
       " 1108784214,\n",
       " 2277916750,\n",
       " 1554353509,\n",
       " 2433340726,\n",
       " 469324743,\n",
       " 2229141780,\n",
       " 3678957762,\n",
       " 3521408938,\n",
       " 2913854158,\n",
       " 129322904,\n",
       " 3343128287,\n",
       " 2680974614,\n",
       " 445799610,\n",
       " 591341759,\n",
       " 831645482,\n",
       " 3663470586,\n",
       " 3006579876,\n",
       " 906058515,\n",
       " 3535630151,\n",
       " 851232387,\n",
       " 2790211946,\n",
       " 1304891860,\n",
       " 3100681168,\n",
       " 46477864,\n",
       " 3886239436,\n",
       " 143581679,\n",
       " 3035606688,\n",
       " 2545641802,\n",
       " 1463869457,\n",
       " 1623204954,\n",
       " 1345872690,\n",
       " 199768831,\n",
       " 2451274332,\n",
       " 3537721571,\n",
       " 214472517,\n",
       " 1562776795,\n",
       " 1609198551,\n",
       " 4220947407,\n",
       " 754609289,\n",
       " 2833258159,\n",
       " 3224095383,\n",
       " 879900549,\n",
       " 1675289673,\n",
       " 1020922157,\n",
       " 2509729954,\n",
       " 3303219960,\n",
       " 1654275949,\n",
       " 533903383,\n",
       " 836257604,\n",
       " 224785048,\n",
       " 1852978590,\n",
       " 1392137160,\n",
       " 2870815704,\n",
       " 1489690189,\n",
       " 2769653750,\n",
       " 122242041,\n",
       " 3474569261,\n",
       " 624614622,\n",
       " 2766016334,\n",
       " 772994711,\n",
       " 876527146,\n",
       " 1733149984,\n",
       " 1727178279,\n",
       " 2272739023,\n",
       " 2976772979,\n",
       " 1470609814,\n",
       " 3829124831,\n",
       " 1223197966,\n",
       " 3189901730,\n",
       " 115770998,\n",
       " 1718348096,\n",
       " 1944543792,\n",
       " 530590498,\n",
       " 1153846260,\n",
       " 1164840442,\n",
       " 1396217517,\n",
       " 3148735767,\n",
       " 2544251833,\n",
       " 2506852398,\n",
       " 1607260189,\n",
       " 3812270872,\n",
       " 4255281026,\n",
       " 2071655458,\n",
       " 1173393405,\n",
       " 902581728,\n",
       " 3742810130,\n",
       " 1881298608,\n",
       " 1441929509,\n",
       " 1083140743,\n",
       " 2238125585,\n",
       " 152516027,\n",
       " 793524260,\n",
       " 326007458,\n",
       " 3803690664,\n",
       " 776623154,\n",
       " 1120602616,\n",
       " 1266337522,\n",
       " 1517603418,\n",
       " 3256990923,\n",
       " 3240427740,\n",
       " 2973372080,\n",
       " 1100956021,\n",
       " 2114307905,\n",
       " 442380991,\n",
       " 175306829,\n",
       " 208285149,\n",
       " 961847959,\n",
       " 1981588152,\n",
       " 2747050393,\n",
       " 4088778984,\n",
       " 634364950,\n",
       " 3305071905,\n",
       " 697413024,\n",
       " 3487015985,\n",
       " 2252464318,\n",
       " 3768657831,\n",
       " 4246143600,\n",
       " 3300697189,\n",
       " 2310084527,\n",
       " 3912946669,\n",
       " 1444607283,\n",
       " 4171633348,\n",
       " 4158353833,\n",
       " 869117413,\n",
       " 1161725843,\n",
       " 411664203,\n",
       " 283929445,\n",
       " 1654065685,\n",
       " 526324076,\n",
       " 2439691824,\n",
       " 2583633551,\n",
       " 1857560241,\n",
       " 4208533465,\n",
       " 3476787014,\n",
       " 1080427065,\n",
       " 1954884277,\n",
       " 1294561301,\n",
       " 1814747521,\n",
       " 3540529368,\n",
       " 1321309439,\n",
       " 2775743578,\n",
       " 2489418277,\n",
       " 2934717587,\n",
       " 3406169315,\n",
       " 2232425085,\n",
       " 2833548284,\n",
       " 258427019,\n",
       " 2390726320,\n",
       " 666885980,\n",
       " 2327699018,\n",
       " 3851351587,\n",
       " 1293639988,\n",
       " 3121511124,\n",
       " 2857172828,\n",
       " 877259653,\n",
       " 3508230284,\n",
       " 56255891,\n",
       " 3132584519,\n",
       " 442311753,\n",
       " 2358725209,\n",
       " 2578605852,\n",
       " 2913199909,\n",
       " 2244746042,\n",
       " 3298192099,\n",
       " 2083906650,\n",
       " 804484594,\n",
       " 529507580,\n",
       " 388443292,\n",
       " 3552401724,\n",
       " 2631124073,\n",
       " 1581063550,\n",
       " 3629281346,\n",
       " 1639570568,\n",
       " 2778238105,\n",
       " 168657923,\n",
       " 1804047341,\n",
       " 18808808,\n",
       " 366280965,\n",
       " 1993327551,\n",
       " 919459719,\n",
       " 4240965641,\n",
       " 2792472621,\n",
       " 638605291,\n",
       " 2028911170,\n",
       " 3002828683,\n",
       " 2119495744,\n",
       " 1160029948,\n",
       " 4064222056,\n",
       " 2818653617,\n",
       " 1639509043,\n",
       " 3310768138,\n",
       " 2555045659,\n",
       " 2948724133,\n",
       " 1701660927,\n",
       " 2840631810,\n",
       " 1872998198,\n",
       " 1772410056,\n",
       " 4085568888,\n",
       " 1057377193,\n",
       " 2690840013,\n",
       " 3992832395,\n",
       " 2218412119,\n",
       " 2580822831,\n",
       " 944915012,\n",
       " 2788521470,\n",
       " 3255875127,\n",
       " 2751113477,\n",
       " 3844012643,\n",
       " 3757005662,\n",
       " 2620335448,\n",
       " 1581770567,\n",
       " 3505792480,\n",
       " 2507086431,\n",
       " 3664722681,\n",
       " 4181834416,\n",
       " 4060966698,\n",
       " 1920497300,\n",
       " 3555067137,\n",
       " 2426076612,\n",
       " 1820776580,\n",
       " 281557144,\n",
       " 3931288141,\n",
       " 2690504094,\n",
       " 262464092,\n",
       " 2603037834,\n",
       " 1938606686,\n",
       " 354341768,\n",
       " 2540319877,\n",
       " 3368829685,\n",
       " 1545599342,\n",
       " 2515722684,\n",
       " 3792224845,\n",
       " 146635627,\n",
       " 2438092286,\n",
       " 968348793,\n",
       " 1317058549,\n",
       " 2996091428,\n",
       " 2801889870,\n",
       " 4070271261,\n",
       " 2417060527,\n",
       " 138452643,\n",
       " 806748732,\n",
       " 519156344,\n",
       " 1490182537,\n",
       " 1683213514,\n",
       " 3283745165,\n",
       " 1391576581,\n",
       " 2425553354,\n",
       " 2102245897,\n",
       " 4048580991,\n",
       " 3419006117,\n",
       " 223336409,\n",
       " 1400827810,\n",
       " 3317194155,\n",
       " 178496767,\n",
       " 4035795784,\n",
       " 1491893881,\n",
       " 3014987853,\n",
       " 85564479,\n",
       " 24929128,\n",
       " 1480624491,\n",
       " 145668310,\n",
       " 3770238924,\n",
       " 2683433667,\n",
       " 2828889229,\n",
       " 109418784,\n",
       " 4002453467,\n",
       " 1918315592,\n",
       " 993025369,\n",
       " 4097404566,\n",
       " 3568695821,\n",
       " 400239024,\n",
       " 4156911339,\n",
       " 982342947,\n",
       " 3205226556,\n",
       " 1356591596,\n",
       " 2493455551,\n",
       " 824200901,\n",
       " 1789413716,\n",
       " 2622586349,\n",
       " 3138012260,\n",
       " 2390488853,\n",
       " 1813181099,\n",
       " 914758409,\n",
       " 1727733977,\n",
       " 4243293594,\n",
       " 1638738470,\n",
       " 3186521353,\n",
       " 680449804,\n",
       " 1306684376,\n",
       " 3075083175,\n",
       " 3392432678,\n",
       " 3627595947,\n",
       " 365903387,\n",
       " 1000072728,\n",
       " 3438451834,\n",
       " 1707265413,\n",
       " 391687031,\n",
       " 1485213300,\n",
       " 650075366,\n",
       " 2368017525,\n",
       " 1137387561,\n",
       " 81994059,\n",
       " 1708283607,\n",
       " 134494787,\n",
       " 3919927325,\n",
       " 1816704057,\n",
       " 1841409501,\n",
       " 964027214,\n",
       " 3342836943,\n",
       " 153993686,\n",
       " 2366018650,\n",
       " 236436648,\n",
       " 986137966,\n",
       " 3961602361,\n",
       " 3830475077,\n",
       " 1090514778,\n",
       " 3396030479,\n",
       " 1050550596,\n",
       " 1349739853,\n",
       " 3639403718,\n",
       " 2960733598,\n",
       " 2641926556,\n",
       " 765259633,\n",
       " 3739843749,\n",
       " 1755605905,\n",
       " 2837483350,\n",
       " 3050299623,\n",
       " 1080467874,\n",
       " 351668652,\n",
       " 3086743532,\n",
       " 819511556,\n",
       " 3476966267,\n",
       " 914202012,\n",
       " 3244846218,\n",
       " 3640052590,\n",
       " 2287476112,\n",
       " 3498875235,\n",
       " 4114960520,\n",
       " 3086650838,\n",
       " 2969587803,\n",
       " 3057839026,\n",
       " 2051306907,\n",
       " 4110518045,\n",
       " 2330526789,\n",
       " 1525619610,\n",
       " 3436072202,\n",
       " 1133339901,\n",
       " 1768988693,\n",
       " 2129035347,\n",
       " 987919236,\n",
       " 662976173,\n",
       " 1406475019,\n",
       " 827308566,\n",
       " 857321527,\n",
       " 936362966,\n",
       " 4213449423,\n",
       " 421152103,\n",
       " 3037958884,\n",
       " 3623248732,\n",
       " 249089676,\n",
       " 2124199546,\n",
       " 127807841,\n",
       " 2459957297,\n",
       " 1395314286,\n",
       " 3390299618,\n",
       " 3695535136,\n",
       " 1831003248,\n",
       " 2891666524,\n",
       " 4150613751,\n",
       " 1357396004,\n",
       " 1782399579,\n",
       " 1627139768,\n",
       " 316337579,\n",
       " 2098406135,\n",
       " 3031341545,\n",
       " 3809451061,\n",
       " 2415089190,\n",
       " 3782163506,\n",
       " 3854235984,\n",
       " 2922402160,\n",
       " 106890502,\n",
       " 1330378793,\n",
       " 623427614,\n",
       " 2284638469,\n",
       " 1600729157,\n",
       " 2989088282,\n",
       " 4093838058,\n",
       " 3894944927,\n",
       " 3766397887,\n",
       " 541052307,\n",
       " 2625754189,\n",
       " 2827434915,\n",
       " 2534199463,\n",
       " 1857093009,\n",
       " 3377265865,\n",
       " 1753638973,\n",
       " 436738150,\n",
       " 88253978,\n",
       " 2841659354,\n",
       " 3885813327,\n",
       " 3694164052,\n",
       " 3990868348,\n",
       " 3230519424,\n",
       " 4096845014,\n",
       " 489113849,\n",
       " 2303705085,\n",
       " 2203180732,\n",
       " 1614810741,\n",
       " 869492426,\n",
       " 205367822,\n",
       " 3483277265,\n",
       " 2586237695,\n",
       " 1397119436,\n",
       " 2995696054,\n",
       " 2813775122,\n",
       " 1529997959,\n",
       " 2807866768,\n",
       " 647049887,\n",
       " 2449911945,\n",
       " 4015888664,\n",
       " 397971972,\n",
       " 1856537438,\n",
       " 2612469624,\n",
       " 2719485790,\n",
       " 1829007318,\n",
       " 1954649122,\n",
       " 1616728358,\n",
       " 3320685330,\n",
       " 1024445418,\n",
       " 2471798771,\n",
       " 414774097,\n",
       " 1316070907,\n",
       " 2372283108,\n",
       " 664441872,\n",
       " 1909050332,\n",
       " 3245971504,\n",
       " 1825684861,\n",
       " 3200721703,\n",
       " 3216436809,\n",
       " 700385824,\n",
       " 3408329340,\n",
       " 3807000147,\n",
       " 265802662,\n",
       " 4059069595,\n",
       " 987685903,\n",
       " 1943861790,\n",
       " 1987922381,\n",
       " 3455716649,\n",
       " 2428616301,\n",
       " 3894250566,\n",
       " 2275169588,\n",
       " 3372268692,\n",
       " 3800665766,\n",
       " 727439599,\n",
       " 3230101954,\n",
       " 3624275837,\n",
       " 2954258572,\n",
       " 4188978889,\n",
       " 4272558997,\n",
       " 1737299864,\n",
       " 765087384,\n",
       " 152410112,\n",
       " 3656487597,\n",
       " 2939260164,\n",
       " 2707409318,\n",
       " 904007375,\n",
       " 1834385658,\n",
       " 3692658155,\n",
       " 719583929,\n",
       " 3361627728,\n",
       " 3735130715,\n",
       " 3180554674,\n",
       " 1913612206,\n",
       " 2825208989,\n",
       " 2426166962,\n",
       " 533744537,\n",
       " 2064225484,\n",
       " 3172603591,\n",
       " 1521133260,\n",
       " 3698940973,\n",
       " 779558633,\n",
       " 1968373399,\n",
       " 2807989169,\n",
       " 1476914244,\n",
       " 126241979,\n",
       " 3571154572,\n",
       " 152771385,\n",
       " 2644339679,\n",
       " 1576457443,\n",
       " 564394007,\n",
       " 3161001899,\n",
       " 3056290559,\n",
       " 4002661168,\n",
       " 1063919001,\n",
       " 3275075783,\n",
       " 3538303785,\n",
       " 2473898455,\n",
       " 1184678515,\n",
       " 3555294645,\n",
       " 4169966668,\n",
       " 10677500,\n",
       " 2912207265,\n",
       " 3598609801,\n",
       " 1765665425,\n",
       " 2466921969,\n",
       " 2095862218,\n",
       " 2053412733,\n",
       " 947242755,\n",
       " 1344314468,\n",
       " 3417485313,\n",
       " 2115232590,\n",
       " 1053924663,\n",
       " 839617645,\n",
       " 1963574037,\n",
       " 551128503,\n",
       " 3583493112,\n",
       " 2800504144,\n",
       " 2091857942,\n",
       " 102240862,\n",
       " 965386498,\n",
       " 1202419561,\n",
       " 1107217185,\n",
       " 3412254741,\n",
       " 2858306676,\n",
       " 3526301481,\n",
       " 631302513,\n",
       " 650784539,\n",
       " 2749521952,\n",
       " 595312976,\n",
       " 2658602129,\n",
       " 1273846876,\n",
       " 612155260,\n",
       " 11958211,\n",
       " 1586980685,\n",
       " 897189818,\n",
       " 3815153324,\n",
       " 3044330948,\n",
       " 2777430062,\n",
       " 523099813,\n",
       " 3911492056,\n",
       " 380945119,\n",
       " 2469240362,\n",
       " 3610302086,\n",
       " 2028091687,\n",
       " 2906843836,\n",
       " 1966662252,\n",
       " 3395447994,\n",
       " 1494985927,\n",
       " 3043542793,\n",
       " 3311876449,\n",
       " 3229951132,\n",
       " 715466250,\n",
       " 2907411865,\n",
       " 2470136498,\n",
       " 1810049544,\n",
       " 458610344,\n",
       " 713889370,\n",
       " 2916833136,\n",
       " 1225451282,\n",
       " 2449416993,\n",
       " 1655959703,\n",
       " 1599113945,\n",
       " 2166332522,\n",
       " 1613869066,\n",
       " 2765896941,\n",
       " 1652833905,\n",
       " 3471216149,\n",
       " 2295809479,\n",
       " 1186300322,\n",
       " 4026446626,\n",
       " 2447234431,\n",
       " 2698429554,\n",
       " 2679655658,\n",
       " 3337752627,\n",
       " 2122290907,\n",
       " 1589035453,\n",
       " 2686585864,\n",
       " 2350486324,\n",
       " 2553821276,\n",
       " 1512513407,\n",
       " 3817105661,\n",
       " 3836014600,\n",
       " 673810473,\n",
       " 2125570712,\n",
       " 2590590469,\n",
       " 98246672,\n",
       " 206378606,\n",
       " 2492861301,\n",
       " 1846414212,\n",
       " 247297393,\n",
       " 3367153204,\n",
       " 3375981280,\n",
       " 2787764413,\n",
       " 472332871,\n",
       " 3314258940,\n",
       " 803261538,\n",
       " 2275803205,\n",
       " 3729029867,\n",
       " 2085238949,\n",
       " 2222103536,\n",
       " 1584342704,\n",
       " 2015290432,\n",
       " 1997541267,\n",
       " 1161798541,\n",
       " 1278589676,\n",
       " 1637119401,\n",
       " 2896380825,\n",
       " 2741729736,\n",
       " 2909506321,\n",
       " 2877092689,\n",
       " 4146499777,\n",
       " 1623014403,\n",
       " 1562938169,\n",
       " 1241782273,\n",
       " 1355706627,\n",
       " 3504720615,\n",
       " 3655925793,\n",
       " 3108180317,\n",
       " 1107943409,\n",
       " 4048724686,\n",
       " 3568021725,\n",
       " 542019185,\n",
       " 1545470171,\n",
       " 2930272703,\n",
       " 4049568648,\n",
       " 2848503706,\n",
       " 4144407314,\n",
       " 5933092,\n",
       " 2853649600,\n",
       " 3325712886,\n",
       " 2550565597,\n",
       " 3293065013,\n",
       " 4196260882,\n",
       " 455800430,\n",
       " 3053552104,\n",
       " 2723534427,\n",
       " 1725360717,\n",
       " 3551863575,\n",
       " 3458450965,\n",
       " 2562087171,\n",
       " 2705739110,\n",
       " 2809025151,\n",
       " 2342783224,\n",
       " 4255698833,\n",
       " 3731110590,\n",
       " 1516335387,\n",
       " 2449528899,\n",
       " 3085439840,\n",
       " 1688111040,\n",
       " 3841473535,\n",
       " 421130099,\n",
       " 160089529,\n",
       " 2455423044,\n",
       " 2222091,\n",
       " 1185576100,\n",
       " 3802502817,\n",
       " 2679459997,\n",
       " 2971659992,\n",
       " 315445283,\n",
       " 1301815019,\n",
       " 1035478722,\n",
       " 2334494639,\n",
       " 459514953,\n",
       " 3817879923,\n",
       " 1923937382,\n",
       " 3599675668,\n",
       " 1935247738,\n",
       " 1353089251,\n",
       " 3861401826,\n",
       " 45252470,\n",
       " 2731537157,\n",
       " 1892169638,\n",
       " 4061603383,\n",
       " 527307359,\n",
       " 3180540355,\n",
       " 1067427764,\n",
       " 2202222387,\n",
       " 8953462,\n",
       " 3729270356,\n",
       " 1283922150,\n",
       " 1879720036,\n",
       " 24385490,\n",
       " 2992874815,\n",
       " 2296766818,\n",
       " 280204479,\n",
       " 3855995928,\n",
       " 2950944901,\n",
       " 2169114842,\n",
       " 12041189,\n",
       " 565454504,\n",
       " 2236590854,\n",
       " 1664443668,\n",
       " 755823229,\n",
       " 3437640451,\n",
       " 1897417193,\n",
       " 1807263018,\n",
       " 4214850965,\n",
       " 4068869449,\n",
       " 380926837,\n",
       " 1360776039,\n",
       " 2081122489,\n",
       " 1345134114,\n",
       " 1772604542,\n",
       " 4110819638,\n",
       " 3676691646,\n",
       " 1414737118,\n",
       " 791939768,\n",
       " 1837602801,\n",
       " 2191023152,\n",
       " 144866040,\n",
       " 3983290503,\n",
       " 593475634,\n",
       " 3241520863,\n",
       " 812376815,\n",
       " 1246047050,\n",
       " 1779485216,\n",
       " 2316544095,\n",
       " 955951537,\n",
       " 3556306241,\n",
       " 1908476808,\n",
       " 3266138511,\n",
       " 127389297,\n",
       " 304396987,\n",
       " 587411276,\n",
       " 1199545770,\n",
       " 1180756749,\n",
       " 2187150599,\n",
       " 3715599914,\n",
       " 396655365,\n",
       " 1378167057,\n",
       " 2478780980,\n",
       " 2008904875,\n",
       " 2367129377,\n",
       " 975124937,\n",
       " 2796705902,\n",
       " 1806203761,\n",
       " 3814193160,\n",
       " 2938889177,\n",
       " 144918362,\n",
       " 3716071832,\n",
       " 4191651791,\n",
       " 3833813644,\n",
       " 244143540,\n",
       " 3816110418,\n",
       " 2956499528,\n",
       " 2088056817,\n",
       " 3464418194,\n",
       " 3058580660,\n",
       " 1465949772,\n",
       " 2799475656,\n",
       " 3048326863,\n",
       " 728794183,\n",
       " 215775835,\n",
       " 1444612301,\n",
       " 2287316751,\n",
       " 2790916594,\n",
       " 3286343422,\n",
       " 819984966,\n",
       " 4115212267,\n",
       " 460850727,\n",
       " 4131616562,\n",
       " 3225654161,\n",
       " 3888072735,\n",
       " 1885211759,\n",
       " 4055855007,\n",
       " 3155817261,\n",
       " 560385329,\n",
       " 3701959001,\n",
       " 2846599791,\n",
       " 3765259762,\n",
       " 131864238,\n",
       " 1065351105,\n",
       " 4065440982,\n",
       " 359016175,\n",
       " 4258078361,\n",
       " 3737994988,\n",
       " 2925204661,\n",
       " 3641804428,\n",
       " 2077343198,\n",
       " 4125257086,\n",
       " 1023476371,\n",
       " 2264653104,\n",
       " 1864782244,\n",
       " 3016051926,\n",
       " 2286152794,\n",
       " 2303400556,\n",
       " 2460559749,\n",
       " 143009361,\n",
       " 3919014256,\n",
       " 420634248,\n",
       " 2355003383,\n",
       " 278455973,\n",
       " 1151858219,\n",
       " 1574635315,\n",
       " 1787915656,\n",
       " 2810182548,\n",
       " 1710850334,\n",
       " 1240310190,\n",
       " 3553755539,\n",
       " 716439187,\n",
       " 3700453074,\n",
       " 2040958485,\n",
       " 1854939487,\n",
       " 1720018602,\n",
       " 1225105778,\n",
       " 2055350354,\n",
       " 3993055953,\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uselabel = train.drop_duplicates(subset = ['eeg_id']).label_id.tolist()\n",
    "uselabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6df714b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16064\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>eeg_sub_id</th>\n",
       "      <th>eeg_label_offset_seconds</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>spectrogram_sub_id</th>\n",
       "      <th>spectrogram_label_offset_seconds</th>\n",
       "      <th>label_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expert_consensus</th>\n",
       "      <th>total_evaluators</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GPD</th>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRDA</th>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LPD</th>\n",
       "      <th>4</th>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LRDA</th>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <th>4</th>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seizure</th>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   eeg_id  eeg_sub_id  \\\n",
       "expert_consensus total_evaluators                       \n",
       "GPD              4                     59          59   \n",
       "GRDA             4                      6           6   \n",
       "LPD              4                    133         133   \n",
       "LRDA             4                     14          14   \n",
       "Other            4                     86          86   \n",
       "Seizure          4                     66          66   \n",
       "\n",
       "                                   eeg_label_offset_seconds  spectrogram_id  \\\n",
       "expert_consensus total_evaluators                                             \n",
       "GPD              4                                       59              59   \n",
       "GRDA             4                                        6               6   \n",
       "LPD              4                                      133             133   \n",
       "LRDA             4                                       14              14   \n",
       "Other            4                                       86              86   \n",
       "Seizure          4                                       66              66   \n",
       "\n",
       "                                   spectrogram_sub_id  \\\n",
       "expert_consensus total_evaluators                       \n",
       "GPD              4                                 59   \n",
       "GRDA             4                                  6   \n",
       "LPD              4                                133   \n",
       "LRDA             4                                 14   \n",
       "Other            4                                 86   \n",
       "Seizure          4                                 66   \n",
       "\n",
       "                                   spectrogram_label_offset_seconds  label_id  \\\n",
       "expert_consensus total_evaluators                                               \n",
       "GPD              4                                               59        59   \n",
       "GRDA             4                                                6         6   \n",
       "LPD              4                                              133       133   \n",
       "LRDA             4                                               14        14   \n",
       "Other            4                                               86        86   \n",
       "Seizure          4                                               66        66   \n",
       "\n",
       "                                   patient_id  seizure_vote  lpd_vote  \\\n",
       "expert_consensus total_evaluators                                       \n",
       "GPD              4                         59            59        59   \n",
       "GRDA             4                          6             6         6   \n",
       "LPD              4                        133           133       133   \n",
       "LRDA             4                         14            14        14   \n",
       "Other            4                         86            86        86   \n",
       "Seizure          4                         66            66        66   \n",
       "\n",
       "                                   gpd_vote  lrda_vote  grda_vote  other_vote  \\\n",
       "expert_consensus total_evaluators                                               \n",
       "GPD              4                       59         59         59          59   \n",
       "GRDA             4                        6          6          6           6   \n",
       "LPD              4                      133        133        133         133   \n",
       "LRDA             4                       14         14         14          14   \n",
       "Other            4                       86         86         86          86   \n",
       "Seizure          4                       66         66         66          66   \n",
       "\n",
       "                                   target  \n",
       "expert_consensus total_evaluators          \n",
       "GPD              4                     59  \n",
       "GRDA             4                      6  \n",
       "LPD              4                    133  \n",
       "LRDA             4                     14  \n",
       "Other            4                     86  \n",
       "Seizure          4                     66  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[train[\"label_id\"].isin(uselabel)].copy()\n",
    "print(train.shape[0])\n",
    "\n",
    "y_data = train[TARGETS].values + 0  # Regularization value\n",
    "y_data = y_data / y_data.sum(axis=1, keepdims=True)\n",
    "train[TARGETS] = y_data\n",
    "\n",
    "train[\"target\"] = train[\"expert_consensus\"]\n",
    "\n",
    "train[train['total_evaluators'] == CFG.test_total_eval].groupby(['expert_consensus','total_evaluators']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05750538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10390\n",
      "5233\n"
     ]
    }
   ],
   "source": [
    "if CFG.test_total_eval > 0:\n",
    "    train['key_id'] = range(train.shape[0])\n",
    "\n",
    "    train_pop_olds = []\n",
    "    for total_eval in CFG.total_evals_old:\n",
    "        if type(total_eval) is list:\n",
    "            pop_idx = (train[\"total_evaluators\"] >= total_eval[0][0]) & (\n",
    "                train[\"total_evaluators\"] < total_eval[0][1]\n",
    "            ) | (train[\"total_evaluators\"] >= total_eval[1][0]) & (\n",
    "                train[\"total_evaluators\"] < total_eval[1][1]\n",
    "            )\n",
    "        else:\n",
    "            pop_idx = (train[\"total_evaluators\"] >= total_eval[0]) & (\n",
    "                train[\"total_evaluators\"] < total_eval[1]\n",
    "            )\n",
    "\n",
    "        train_pop = train[pop_idx].copy().reset_index()\n",
    "\n",
    "        sgkf = KFold(n_splits=CFG.n_fold)\n",
    "        train_pop[\"fold\"] = -1\n",
    "        for fold_id, (_, val_idx) in enumerate(\n",
    "            sgkf.split(train_pop)\n",
    "        ):\n",
    "            train_pop.loc[val_idx, \"fold\"] = fold_id\n",
    "        train_pop_olds.append(train_pop)\n",
    "        print(train_pop.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d184b06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10772\n",
      "5233\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>eeg_sub_id</th>\n",
       "      <th>eeg_label_offset_seconds</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>spectrogram_sub_id</th>\n",
       "      <th>spectrogram_label_offset_seconds</th>\n",
       "      <th>label_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>target</th>\n",
       "      <th>key_id</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expert_consensus</th>\n",
       "      <th>total_evaluators</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRDA</th>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LPD</th>\n",
       "      <th>4</th>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LRDA</th>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <th>4</th>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seizure</th>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   index  eeg_id  eeg_sub_id  \\\n",
       "expert_consensus total_evaluators                              \n",
       "GRDA             4                     6       6           6   \n",
       "LPD              4                   133     133         133   \n",
       "LRDA             4                    14      14          14   \n",
       "Other            4                    86      86          86   \n",
       "Seizure          4                    66      66          66   \n",
       "\n",
       "                                   eeg_label_offset_seconds  spectrogram_id  \\\n",
       "expert_consensus total_evaluators                                             \n",
       "GRDA             4                                        6               6   \n",
       "LPD              4                                      133             133   \n",
       "LRDA             4                                       14              14   \n",
       "Other            4                                       86              86   \n",
       "Seizure          4                                       66              66   \n",
       "\n",
       "                                   spectrogram_sub_id  \\\n",
       "expert_consensus total_evaluators                       \n",
       "GRDA             4                                  6   \n",
       "LPD              4                                133   \n",
       "LRDA             4                                 14   \n",
       "Other            4                                 86   \n",
       "Seizure          4                                 66   \n",
       "\n",
       "                                   spectrogram_label_offset_seconds  label_id  \\\n",
       "expert_consensus total_evaluators                                               \n",
       "GRDA             4                                                6         6   \n",
       "LPD              4                                              133       133   \n",
       "LRDA             4                                               14        14   \n",
       "Other            4                                               86        86   \n",
       "Seizure          4                                               66        66   \n",
       "\n",
       "                                   patient_id  seizure_vote  lpd_vote  \\\n",
       "expert_consensus total_evaluators                                       \n",
       "GRDA             4                          6             6         6   \n",
       "LPD              4                        133           133       133   \n",
       "LRDA             4                         14            14        14   \n",
       "Other            4                         86            86        86   \n",
       "Seizure          4                         66            66        66   \n",
       "\n",
       "                                   gpd_vote  lrda_vote  grda_vote  other_vote  \\\n",
       "expert_consensus total_evaluators                                               \n",
       "GRDA             4                        6          6          6           6   \n",
       "LPD              4                      133        133        133         133   \n",
       "LRDA             4                       14         14         14          14   \n",
       "Other            4                       86         86         86          86   \n",
       "Seizure          4                       66         66         66          66   \n",
       "\n",
       "                                   target  key_id  fold  \n",
       "expert_consensus total_evaluators                        \n",
       "GRDA             4                      6       6     6  \n",
       "LPD              4                    133     133   133  \n",
       "LRDA             4                     14      14    14  \n",
       "Other            4                     86      86    86  \n",
       "Seizure          4                     66      66    66  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pops = []\n",
    "for eval_list in CFG.total_evaluators:\n",
    "    result=[]\n",
    "    train_pop = train  \n",
    "    for eval_dict in eval_list:\n",
    "        band = eval_dict['band']\n",
    "        pop_idx = (train_pop[\"total_evaluators\"] >= band[0]) \n",
    "        pop_idx &= (train_pop[\"total_evaluators\"] <= band[1])\n",
    "        for exclude in eval_dict['excl_evals']:\n",
    "            pop_idx &= ~(train_pop['expert_consensus'] == exclude)\n",
    "            pass\n",
    "        result.append(train_pop[pop_idx])\n",
    "    train_pop = pd.concat(result).copy().reset_index()\n",
    "\n",
    "    sgkf = GroupKFold(n_splits=CFG.n_fold)\n",
    "    train_pop[\"fold\"] = -1\n",
    "    for fold_id, (_, val_idx) in enumerate(\n",
    "        sgkf.split(train_pop, y=train_pop[\"target\"], groups=train_pop[\"patient_id\"])\n",
    "    ):\n",
    "        train_pop.loc[val_idx, \"fold\"] = fold_id\n",
    "\n",
    "    train_pops.append(train_pop)\n",
    "    print(train_pop.shape[0])\n",
    "\n",
    "train_0 = train_pops[0]\n",
    "train_0[train_0['total_evaluators'] == CFG.test_total_eval].groupby(['expert_consensus','total_evaluators']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0346f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>eeg_sub_id</th>\n",
       "      <th>eeg_label_offset_seconds</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>spectrogram_sub_id</th>\n",
       "      <th>spectrogram_label_offset_seconds</th>\n",
       "      <th>label_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>expert_consensus</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>total_evaluators</th>\n",
       "      <th>target</th>\n",
       "      <th>Exist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>1626798710</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1219001</td>\n",
       "      <td>2</td>\n",
       "      <td>74.0</td>\n",
       "      <td>3631726128</td>\n",
       "      <td>23435</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>2529955608</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1219001</td>\n",
       "      <td>4</td>\n",
       "      <td>190.0</td>\n",
       "      <td>4265493714</td>\n",
       "      <td>23435</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>62</td>\n",
       "      <td>989810287</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2843061</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116172961</td>\n",
       "      <td>13521</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5</td>\n",
       "      <td>Other</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>172</td>\n",
       "      <td>4000022002</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7122706</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>462363590</td>\n",
       "      <td>11471</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>458</td>\n",
       "      <td>3636015289</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12849827</td>\n",
       "      <td>30</td>\n",
       "      <td>452.0</td>\n",
       "      <td>640352706</td>\n",
       "      <td>34998</td>\n",
       "      <td>GPD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>GPD</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10719</th>\n",
       "      <td>99802</td>\n",
       "      <td>3453816311</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2122259534</td>\n",
       "      <td>2</td>\n",
       "      <td>536.0</td>\n",
       "      <td>128199276</td>\n",
       "      <td>48272</td>\n",
       "      <td>GPD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>GPD</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10744</th>\n",
       "      <td>99964</td>\n",
       "      <td>1573558442</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2126560277</td>\n",
       "      <td>15</td>\n",
       "      <td>704.0</td>\n",
       "      <td>1311940936</td>\n",
       "      <td>22597</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10783</th>\n",
       "      <td>100317</td>\n",
       "      <td>2024173188</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2136113568</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1857591903</td>\n",
       "      <td>51025</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10815</th>\n",
       "      <td>100616</td>\n",
       "      <td>4288875638</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2145358771</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3377992611</td>\n",
       "      <td>200</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10826</th>\n",
       "      <td>100754</td>\n",
       "      <td>3349371726</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2146170054</td>\n",
       "      <td>7</td>\n",
       "      <td>124.0</td>\n",
       "      <td>142522487</td>\n",
       "      <td>33380</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index      eeg_id  eeg_sub_id  eeg_label_offset_seconds  \\\n",
       "3          30  1626798710           0                       0.0   \n",
       "4          32  2529955608           0                       0.0   \n",
       "11         62   989810287           0                       0.0   \n",
       "31        172  4000022002           0                       0.0   \n",
       "60        458  3636015289           0                       0.0   \n",
       "...       ...         ...         ...                       ...   \n",
       "10719   99802  3453816311           0                       0.0   \n",
       "10744   99964  1573558442           0                       0.0   \n",
       "10783  100317  2024173188           0                       0.0   \n",
       "10815  100616  4288875638           0                       0.0   \n",
       "10826  100754  3349371726           0                       0.0   \n",
       "\n",
       "       spectrogram_id  spectrogram_sub_id  spectrogram_label_offset_seconds  \\\n",
       "3             1219001                   2                              74.0   \n",
       "4             1219001                   4                             190.0   \n",
       "11            2843061                   0                               0.0   \n",
       "31            7122706                   0                               0.0   \n",
       "60           12849827                  30                             452.0   \n",
       "...               ...                 ...                               ...   \n",
       "10719      2122259534                   2                             536.0   \n",
       "10744      2126560277                  15                             704.0   \n",
       "10783      2136113568                   0                               0.0   \n",
       "10815      2145358771                   0                               0.0   \n",
       "10826      2146170054                   7                             124.0   \n",
       "\n",
       "         label_id  patient_id expert_consensus  seizure_vote  lpd_vote  \\\n",
       "3      3631726128       23435          Seizure           0.6       0.0   \n",
       "4      4265493714       23435          Seizure           0.6       0.0   \n",
       "11      116172961       13521            Other           0.0       0.2   \n",
       "31      462363590       11471          Seizure           0.6       0.0   \n",
       "60      640352706       34998              GPD           0.0       0.0   \n",
       "...           ...         ...              ...           ...       ...   \n",
       "10719   128199276       48272              GPD           0.0       0.0   \n",
       "10744  1311940936       22597          Seizure           0.6       0.0   \n",
       "10783  1857591903       51025          Seizure           0.6       0.0   \n",
       "10815  3377992611         200          Seizure           0.8       0.2   \n",
       "10826   142522487       33380          Seizure           0.6       0.0   \n",
       "\n",
       "       gpd_vote  lrda_vote  grda_vote  other_vote  total_evaluators   target  \\\n",
       "3           0.4        0.0        0.0         0.0                 5  Seizure   \n",
       "4           0.4        0.0        0.0         0.0                 5  Seizure   \n",
       "11          0.0        0.0        0.0         0.8                 5    Other   \n",
       "31          0.0        0.0        0.0         0.4                 5  Seizure   \n",
       "60          0.8        0.0        0.0         0.2                 5      GPD   \n",
       "...         ...        ...        ...         ...               ...      ...   \n",
       "10719       1.0        0.0        0.0         0.0                 5      GPD   \n",
       "10744       0.0        0.0        0.0         0.4                 5  Seizure   \n",
       "10783       0.0        0.2        0.0         0.2                 5  Seizure   \n",
       "10815       0.0        0.0        0.0         0.0                 5  Seizure   \n",
       "10826       0.4        0.0        0.0         0.0                 5  Seizure   \n",
       "\n",
       "            Exist  \n",
       "3      right_only  \n",
       "4      right_only  \n",
       "11     right_only  \n",
       "31     right_only  \n",
       "60     right_only  \n",
       "...           ...  \n",
       "10719  right_only  \n",
       "10744  right_only  \n",
       "10783  right_only  \n",
       "10815  right_only  \n",
       "10826  right_only  \n",
       "\n",
       "[500 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CFG.test_total_eval > 0:\n",
    "    df_old = train_pop_olds[0].copy(deep=True).set_index(['key_id'], drop=True).drop(columns=['fold'])\n",
    "    df_new = train_pops[0].copy(deep=True).set_index(['key_id'], drop=True).drop(columns=['fold'])\n",
    "\n",
    "    #outer merge the two DataFrames, adding an indicator column called 'Exist'\n",
    "    diff_df = pd.merge(df_old, df_new, how='outer', indicator='Exist')\n",
    "\n",
    "    #find which rows don't exist in both DataFrames\n",
    "    diff_df = diff_df.loc[diff_df['Exist'] != 'both']\n",
    "    display(diff_df)\n",
    "\n",
    "    del df_old, df_new, diff_df, train_pop_olds\n",
    "    _ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bd2e3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pop 1: train unique eeg_id + votes shape: (10772, 20)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Pop 1: train unique eeg_id + votes shape:\", train_pops[0].shape)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train[\"total_evaluators\"], bins=10, color=\"blue\", edgecolor=\"black\")\n",
    "plt.title(\"Histogram of Total Evaluators\")\n",
    "plt.xlabel(\"Total Evaluators\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c9dffa",
   "metadata": {},
   "source": [
    "# Deduplicate Train EEG Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d0009f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.visualize:\n",
    "    train_dataset = EEGDataset(\n",
    "        train_pops[0], batch_size=CFG.batch_size, eegs=all_eegs, mode=\"train\"\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    output = train_dataset[0]\n",
    "    X, y = output[\"eeg\"], output[\"labels\"]\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "    iot = torch.randn(2, CFG.nsamples, CFG.in_channels)  # .cuda()\n",
    "    model = EEGNet(\n",
    "        kernels=CFG.kernels,\n",
    "        in_channels=CFG.in_channels,\n",
    "        fixed_kernel_size=CFG.fixed_kernel_size,\n",
    "        num_classes=CFG.target_size,\n",
    "        linear_layer_features=CFG.linear_layer_features,\n",
    "    )\n",
    "    output = model(iot)\n",
    "    print(output.shape)\n",
    "\n",
    "    for batch in train_loader:\n",
    "        X = batch.pop(\"eeg\")\n",
    "        y = batch.pop(\"labels\")\n",
    "        for item in range(4):\n",
    "            plt.figure(figsize=(20, 4))\n",
    "            offset = 0\n",
    "            for col in range(X.shape[-1]):\n",
    "                if col != 0:\n",
    "                    offset -= X[item, :, col].min()\n",
    "                plt.plot(\n",
    "                    range(CFG.nsamples),\n",
    "                    X[item, :, col] + offset,\n",
    "                    label=f\"feature {col+1}\",\n",
    "                )\n",
    "                offset += X[item, :, col].max()\n",
    "            tt = f\"{y[col][0]:0.1f}\"\n",
    "            for t in y[col][1:]:\n",
    "                tt += f\", {t:0.1f}\"\n",
    "            plt.title(f\"EEG_Id = {eeg_ids[item]}\\nTarget = {tt}\", size=14)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        break\n",
    "\n",
    "    del iot, model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ef3fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "if CFG.create_eegs:\n",
    "    all_eegs = {}\n",
    "    visualize = 1 if CFG.visualize else 0\n",
    "    eeg_ids = train.eeg_id.unique()\n",
    "\n",
    "    for i, eeg_id in tqdm(enumerate(eeg_ids)):\n",
    "\n",
    "        eeg_path = CFG.path_train / f\"{eeg_id}.parquet\"\n",
    "\n",
    "        data = eeg_from_parquet(eeg_path, display=i < visualize)\n",
    "        all_eegs[eeg_id] = data\n",
    "\n",
    "        if i == visualize:\n",
    "            if CFG.create_eegs:\n",
    "                print(\n",
    "                    f\"Processing {train['eeg_id'].nunique()} eeg parquets... \", end=\"\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Reading {len(eeg_ids)} eeg NumPys from disk.\")\n",
    "                break\n",
    "    np.save(\"./eegs\", all_eegs)\n",
    "\n",
    "else:\n",
    "    all_eegs = np.load(CFG.file_raw_eeg, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ebcde5",
   "metadata": {},
   "source": [
    "# Train Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c57e8912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(preds, targets):\n",
    "    oof = pd.DataFrame(preds.copy())\n",
    "    oof[\"id\"] = np.arange(len(oof))\n",
    "    true = pd.DataFrame(targets.copy())\n",
    "    true[\"id\"] = np.arange(len(true))\n",
    "    cv = kaggle_kl_div.score(solution=true, submission=oof, row_id_column_name=\"id\")\n",
    "    return cv\n",
    "\n",
    "\n",
    "def get_result(result_df):\n",
    "    gt = result_df[[\"eeg_id\"] + CFG.target_cols]\n",
    "    gt.sort_values(by=\"eeg_id\", inplace=True)\n",
    "    gt.reset_index(inplace=True, drop=True)\n",
    "    preds = result_df[[\"eeg_id\"] + CFG.pred_cols]\n",
    "    preds.columns = [\"eeg_id\"] + CFG.target_cols\n",
    "    preds.sort_values(by=\"eeg_id\", inplace=True)\n",
    "    preds.reset_index(inplace=True, drop=True)\n",
    "    score_loss = get_score(gt[CFG.target_cols], preds[CFG.target_cols])\n",
    "    LOGGER.info(f\"Score with best loss weights: {score_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e990ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\" and CFG.train_by_stages:\n",
    "    seed_torch(seed=CFG.seed)\n",
    "\n",
    "    prev_dir = \"\"\n",
    "    for stage in range(len(CFG.total_evaluators)):\n",
    "        pop_dir = f\"{OUTPUT_DIR}pop_{stage+1}_weight_oof/\"\n",
    "        if not os.path.exists(pop_dir):\n",
    "            os.makedirs(pop_dir)\n",
    "\n",
    "        if stage not in CFG.train_stages:\n",
    "            prev_dir = pop_dir\n",
    "            continue\n",
    "\n",
    "        oof_df = pd.DataFrame()\n",
    "        scores = []\n",
    "        for fold in CFG.train_folds:\n",
    "            train_oof_df, score = train_loop(\n",
    "                stage=stage + 1,\n",
    "                epochs=CFG.epochs[stage],\n",
    "                fold=fold,\n",
    "                folds=train_pops[stage],\n",
    "                directory=pop_dir,\n",
    "                prev_dir=prev_dir,\n",
    "                eggs=all_eegs,\n",
    "            )\n",
    "\n",
    "            oof_df = pd.concat([oof_df, train_oof_df])\n",
    "            scores.append(score)\n",
    "\n",
    "            LOGGER.info(f\"========== stage: {stage+1} fold: {fold} result ==========\")\n",
    "            LOGGER.info(f\"Score with best loss weights stage{stage+1}: {score:.4f}\")\n",
    "\n",
    "        LOGGER.info(f\"==================== CV ====================\")\n",
    "        LOGGER.info(f\"Score with best loss weights: {np.mean(scores):.4f}\")\n",
    "\n",
    "        oof_df.reset_index(drop=True, inplace=True)\n",
    "        oof_df.to_csv(\n",
    "            f\"{pop_dir}{CFG.model_name}_oof_df_ver-{CFG.VERSION}_stage-{stage+1}.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "        prev_dir = pop_dir\n",
    "\n",
    "    if CFG.wandb:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b141d539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== stage: 1 fold: 0 training 134 / 17 ==========\n",
      "Epoch 1 Avg Train Loss: 1.3906 Avg Valid Loss: 1.2827 / \n",
      "Epoch 1 Save Best Valid Loss: 1.2827\n",
      "Epoch 2 Avg Train Loss: 1.2219 Avg Valid Loss: 1.1882 / \n",
      "Epoch 2 Save Best Valid Loss: 1.1882\n",
      "Epoch 3 Avg Train Loss: 1.1319 Avg Valid Loss: 1.0155 / \n",
      "Epoch 3 Save Best Valid Loss: 1.0155\n",
      "Epoch 4 Avg Train Loss: 1.0912 Avg Valid Loss: 0.9735 / \n",
      "Epoch 4 Save Best Valid Loss: 0.9735\n",
      "Epoch 5 Avg Train Loss: 1.0469 Avg Valid Loss: 0.9406 / \n",
      "Epoch 5 Save Best Valid Loss: 0.9406\n",
      "Epoch 6 Avg Train Loss: 1.0368 Avg Valid Loss: 0.8935 / \n",
      "Epoch 6 Save Best Valid Loss: 0.8935\n",
      "Epoch 7 Avg Train Loss: 0.9895 Avg Valid Loss: 0.8895 / \n",
      "Epoch 7 Save Best Valid Loss: 0.8895\n",
      "Epoch 8 Avg Train Loss: 0.9847 Avg Valid Loss: 0.8833 / \n",
      "Epoch 8 Save Best Valid Loss: 0.8833\n",
      "Epoch 9 Avg Train Loss: 0.9719 Avg Valid Loss: 0.8543 / \n",
      "Epoch 9 Save Best Valid Loss: 0.8543\n",
      "Epoch 10 Avg Train Loss: 0.9664 Avg Valid Loss: 0.8734 / \n",
      "Epoch 11 Avg Train Loss: 0.9476 Avg Valid Loss: 0.8563 / \n",
      "Epoch 12 Avg Train Loss: 0.9426 Avg Valid Loss: 0.8628 / \n",
      "Epoch 13 Avg Train Loss: 0.9329 Avg Valid Loss: 0.8367 / \n",
      "Epoch 13 Save Best Valid Loss: 0.8367\n",
      "Epoch 14 Avg Train Loss: 0.9255 Avg Valid Loss: 0.8811 / \n",
      "Epoch 15 Avg Train Loss: 0.9153 Avg Valid Loss: 0.8604 / \n",
      "Epoch 16 Avg Train Loss: 0.9052 Avg Valid Loss: 0.8971 / \n",
      "Epoch 17 Avg Train Loss: 0.9146 Avg Valid Loss: 0.8174 / \n",
      "Epoch 17 Save Best Valid Loss: 0.8174\n",
      "Epoch 18 Avg Train Loss: 0.8964 Avg Valid Loss: 0.8044 / \n",
      "Epoch 18 Save Best Valid Loss: 0.8044\n",
      "Epoch 19 Avg Train Loss: 0.9005 Avg Valid Loss: 0.8642 / \n",
      "Epoch 20 Avg Train Loss: 0.8905 Avg Valid Loss: 0.8379 / \n",
      "Epoch 21 Avg Train Loss: 0.8805 Avg Valid Loss: 0.8489 / \n",
      "Epoch 22 Avg Train Loss: 0.8616 Avg Valid Loss: 0.8736 / \n",
      "Epoch 23 Avg Train Loss: 0.8769 Avg Valid Loss: 0.8549 / \n",
      "Epoch 24 Avg Train Loss: 0.8725 Avg Valid Loss: 0.8937 / \n",
      "Epoch 25 Avg Train Loss: 0.8692 Avg Valid Loss: 0.9206 / \n",
      "Epoch 26 Avg Train Loss: 0.8660 Avg Valid Loss: 0.8397 / \n",
      "Epoch 27 Avg Train Loss: 0.8566 Avg Valid Loss: 0.8222 / \n",
      "Epoch 28 Avg Train Loss: 0.8575 Avg Valid Loss: 0.8511 / \n",
      "Epoch 29 Avg Train Loss: 0.8434 Avg Valid Loss: 0.8353 / \n",
      "Epoch 30 Avg Train Loss: 0.8501 Avg Valid Loss: 0.8337 / \n",
      "Epoch 31 Avg Train Loss: 0.8428 Avg Valid Loss: 0.8639 / \n",
      "Epoch 32 Avg Train Loss: 0.8536 Avg Valid Loss: 0.8766 / \n",
      "Epoch 33 Avg Train Loss: 0.8377 Avg Valid Loss: 0.8522 / \n",
      "Epoch 34 Avg Train Loss: 0.8304 Avg Valid Loss: 0.8810 / \n",
      "Epoch 35 Avg Train Loss: 0.8447 Avg Valid Loss: 0.8525 / \n",
      "Epoch 36 Avg Train Loss: 0.8463 Avg Valid Loss: 0.8253 / \n",
      "Epoch 37 Avg Train Loss: 0.8364 Avg Valid Loss: 0.8580 / \n",
      "Epoch 38 Avg Train Loss: 0.8225 Avg Valid Loss: 0.8948 / \n",
      "Epoch 39 Avg Train Loss: 0.8332 Avg Valid Loss: 0.8478 / \n",
      "Epoch 40 Avg Train Loss: 0.8128 Avg Valid Loss: 0.8279 / \n",
      "Epoch 41 Avg Train Loss: 0.8338 Avg Valid Loss: 0.8830 / \n",
      "Epoch 42 Avg Train Loss: 0.8304 Avg Valid Loss: 0.9110 / \n",
      "Epoch 43 Avg Train Loss: 0.8108 Avg Valid Loss: 0.8633 / \n",
      "Epoch 44 Avg Train Loss: 0.8229 Avg Valid Loss: 0.8456 / \n",
      "Epoch 45 Avg Train Loss: 0.8254 Avg Valid Loss: 0.8521 / \n",
      "Epoch 46 Avg Train Loss: 0.8152 Avg Valid Loss: 0.8465 / \n",
      "Epoch 47 Avg Train Loss: 0.8322 Avg Valid Loss: 0.8200 / \n",
      "Epoch 48 Avg Train Loss: 0.8030 Avg Valid Loss: 0.8319 / \n",
      "Epoch 49 Avg Train Loss: 0.8121 Avg Valid Loss: 0.8016 / \n",
      "Epoch 49 Save Best Valid Loss: 0.8016\n",
      "Epoch 50 Avg Train Loss: 0.8113 Avg Valid Loss: 0.8296 / \n",
      "========== fold: 0 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 0.8016\n",
      "========== stage: 2 fold: 0 training 65 / 9 ==========\n",
      "Epoch 1 Avg Train Loss: 0.5126 Avg Valid Loss: 0.4600 / \n",
      "Epoch 1 Save Best Valid Loss: 0.4600\n",
      "Epoch 2 Avg Train Loss: 0.4691 Avg Valid Loss: 0.4267 / \n",
      "Epoch 2 Save Best Valid Loss: 0.4267\n",
      "Epoch 3 Avg Train Loss: 0.4573 Avg Valid Loss: 0.4195 / \n",
      "Epoch 3 Save Best Valid Loss: 0.4195\n",
      "Epoch 4 Avg Train Loss: 0.4578 Avg Valid Loss: 0.4347 / \n",
      "Epoch 5 Avg Train Loss: 0.4441 Avg Valid Loss: 0.4188 / \n",
      "Epoch 5 Save Best Valid Loss: 0.4188\n",
      "Epoch 6 Avg Train Loss: 0.4462 Avg Valid Loss: 0.4287 / \n",
      "Epoch 7 Avg Train Loss: 0.4378 Avg Valid Loss: 0.4182 / \n",
      "Epoch 7 Save Best Valid Loss: 0.4182\n",
      "Epoch 8 Avg Train Loss: 0.4389 Avg Valid Loss: 0.4138 / \n",
      "Epoch 8 Save Best Valid Loss: 0.4138\n",
      "Epoch 9 Avg Train Loss: 0.4392 Avg Valid Loss: 0.4637 / \n",
      "Epoch 10 Avg Train Loss: 0.4377 Avg Valid Loss: 0.4291 / \n",
      "Epoch 11 Avg Train Loss: 0.4239 Avg Valid Loss: 0.4201 / \n",
      "Epoch 12 Avg Train Loss: 0.4237 Avg Valid Loss: 0.4216 / \n",
      "Epoch 13 Avg Train Loss: 0.4288 Avg Valid Loss: 0.4148 / \n",
      "Epoch 14 Avg Train Loss: 0.4338 Avg Valid Loss: 0.4265 / \n",
      "Epoch 15 Avg Train Loss: 0.4311 Avg Valid Loss: 0.4181 / \n",
      "Epoch 16 Avg Train Loss: 0.4254 Avg Valid Loss: 0.4147 / \n",
      "Epoch 17 Avg Train Loss: 0.4290 Avg Valid Loss: 0.4177 / \n",
      "Epoch 18 Avg Train Loss: 0.4137 Avg Valid Loss: 0.4254 / \n",
      "Epoch 19 Avg Train Loss: 0.4219 Avg Valid Loss: 0.4317 / \n",
      "Epoch 20 Avg Train Loss: 0.4173 Avg Valid Loss: 0.4307 / \n",
      "Epoch 21 Avg Train Loss: 0.4223 Avg Valid Loss: 0.4184 / \n",
      "Epoch 22 Avg Train Loss: 0.4148 Avg Valid Loss: 0.4210 / \n",
      "Epoch 23 Avg Train Loss: 0.4139 Avg Valid Loss: 0.4366 / \n",
      "Epoch 24 Avg Train Loss: 0.4172 Avg Valid Loss: 0.4374 / \n",
      "Epoch 25 Avg Train Loss: 0.4154 Avg Valid Loss: 0.4222 / \n",
      "Epoch 26 Avg Train Loss: 0.4122 Avg Valid Loss: 0.4348 / \n",
      "Epoch 27 Avg Train Loss: 0.4130 Avg Valid Loss: 0.4199 / \n",
      "Epoch 28 Avg Train Loss: 0.4125 Avg Valid Loss: 0.4193 / \n",
      "Epoch 29 Avg Train Loss: 0.4066 Avg Valid Loss: 0.4182 / \n",
      "Epoch 30 Avg Train Loss: 0.4079 Avg Valid Loss: 0.4271 / \n",
      "Epoch 31 Avg Train Loss: 0.4003 Avg Valid Loss: 0.4344 / \n",
      "Epoch 32 Avg Train Loss: 0.4053 Avg Valid Loss: 0.4357 / \n",
      "Epoch 33 Avg Train Loss: 0.4058 Avg Valid Loss: 0.4908 / \n",
      "Epoch 34 Avg Train Loss: 0.4036 Avg Valid Loss: 0.4148 / \n",
      "Epoch 35 Avg Train Loss: 0.4080 Avg Valid Loss: 0.4437 / \n",
      "Epoch 36 Avg Train Loss: 0.4032 Avg Valid Loss: 0.4462 / \n",
      "Epoch 37 Avg Train Loss: 0.4011 Avg Valid Loss: 0.4264 / \n",
      "Epoch 38 Avg Train Loss: 0.3993 Avg Valid Loss: 0.4401 / \n",
      "Epoch 39 Avg Train Loss: 0.4080 Avg Valid Loss: 0.4253 / \n",
      "Epoch 40 Avg Train Loss: 0.4034 Avg Valid Loss: 0.4332 / \n",
      "Epoch 41 Avg Train Loss: 0.4002 Avg Valid Loss: 0.4163 / \n",
      "Epoch 42 Avg Train Loss: 0.3924 Avg Valid Loss: 0.4385 / \n",
      "Epoch 43 Avg Train Loss: 0.4055 Avg Valid Loss: 0.4215 / \n",
      "Epoch 44 Avg Train Loss: 0.3958 Avg Valid Loss: 0.4261 / \n",
      "Epoch 45 Avg Train Loss: 0.4016 Avg Valid Loss: 0.4383 / \n",
      "Epoch 46 Avg Train Loss: 0.4028 Avg Valid Loss: 0.4382 / \n",
      "Epoch 47 Avg Train Loss: 0.3994 Avg Valid Loss: 0.4443 / \n",
      "Epoch 48 Avg Train Loss: 0.3956 Avg Valid Loss: 0.4261 / \n",
      "Epoch 49 Avg Train Loss: 0.3897 Avg Valid Loss: 0.4423 / \n",
      "Epoch 50 Avg Train Loss: 0.3959 Avg Valid Loss: 0.4418 / \n",
      "Epoch 51 Avg Train Loss: 0.4050 Avg Valid Loss: 0.4293 / \n",
      "Epoch 52 Avg Train Loss: 0.3929 Avg Valid Loss: 0.4246 / \n",
      "Epoch 53 Avg Train Loss: 0.3965 Avg Valid Loss: 0.4555 / \n",
      "Epoch 54 Avg Train Loss: 0.3931 Avg Valid Loss: 0.4202 / \n",
      "Epoch 55 Avg Train Loss: 0.3990 Avg Valid Loss: 0.4171 / \n",
      "Epoch 56 Avg Train Loss: 0.3977 Avg Valid Loss: 0.4464 / \n",
      "Epoch 57 Avg Train Loss: 0.3976 Avg Valid Loss: 0.4926 / \n",
      "Epoch 58 Avg Train Loss: 0.3920 Avg Valid Loss: 0.4604 / \n",
      "Epoch 59 Avg Train Loss: 0.3912 Avg Valid Loss: 0.4519 / \n",
      "Epoch 60 Avg Train Loss: 0.3873 Avg Valid Loss: 0.4294 / \n",
      "Epoch 61 Avg Train Loss: 0.3922 Avg Valid Loss: 0.4451 / \n",
      "Epoch 62 Avg Train Loss: 0.3914 Avg Valid Loss: 0.4866 / \n",
      "Epoch 63 Avg Train Loss: 0.4066 Avg Valid Loss: 0.4278 / \n",
      "Epoch 64 Avg Train Loss: 0.3954 Avg Valid Loss: 0.4437 / \n",
      "Epoch 65 Avg Train Loss: 0.3984 Avg Valid Loss: 0.4470 / \n",
      "Epoch 66 Avg Train Loss: 0.3937 Avg Valid Loss: 0.4256 / \n",
      "Epoch 67 Avg Train Loss: 0.3851 Avg Valid Loss: 0.4376 / \n",
      "Epoch 68 Avg Train Loss: 0.3851 Avg Valid Loss: 0.4568 / \n",
      "Epoch 69 Avg Train Loss: 0.3899 Avg Valid Loss: 0.4501 / \n",
      "Epoch 70 Avg Train Loss: 0.3901 Avg Valid Loss: 0.4371 / \n",
      "Epoch 71 Avg Train Loss: 0.4022 Avg Valid Loss: 0.4531 / \n",
      "Epoch 72 Avg Train Loss: 0.3916 Avg Valid Loss: 0.4291 / \n",
      "Epoch 73 Avg Train Loss: 0.3856 Avg Valid Loss: 0.4486 / \n",
      "Epoch 74 Avg Train Loss: 0.3929 Avg Valid Loss: 0.4493 / \n",
      "Epoch 75 Avg Train Loss: 0.3915 Avg Valid Loss: 0.4444 / \n",
      "Epoch 76 Avg Train Loss: 0.3812 Avg Valid Loss: 0.4331 / \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77 Avg Train Loss: 0.3827 Avg Valid Loss: 0.4638 / \n",
      "Epoch 78 Avg Train Loss: 0.3828 Avg Valid Loss: 0.4213 / \n",
      "Epoch 79 Avg Train Loss: 0.3814 Avg Valid Loss: 0.4574 / \n",
      "Epoch 80 Avg Train Loss: 0.3825 Avg Valid Loss: 0.4455 / \n",
      "Epoch 81 Avg Train Loss: 0.3853 Avg Valid Loss: 0.4305 / \n",
      "Epoch 82 Avg Train Loss: 0.3936 Avg Valid Loss: 0.4474 / \n",
      "Epoch 83 Avg Train Loss: 0.3899 Avg Valid Loss: 0.4315 / \n",
      "Epoch 84 Avg Train Loss: 0.3823 Avg Valid Loss: 0.4356 / \n",
      "Epoch 85 Avg Train Loss: 0.3861 Avg Valid Loss: 0.5299 / \n",
      "Epoch 86 Avg Train Loss: 0.3840 Avg Valid Loss: 0.4671 / \n",
      "Epoch 87 Avg Train Loss: 0.3819 Avg Valid Loss: 0.4675 / \n",
      "Epoch 88 Avg Train Loss: 0.3830 Avg Valid Loss: 0.4434 / \n",
      "Epoch 89 Avg Train Loss: 0.3837 Avg Valid Loss: 0.4849 / \n",
      "Epoch 90 Avg Train Loss: 0.3840 Avg Valid Loss: 0.4489 / \n",
      "Epoch 91 Avg Train Loss: 0.3808 Avg Valid Loss: 0.4384 / \n",
      "Epoch 92 Avg Train Loss: 0.3872 Avg Valid Loss: 0.4466 / \n",
      "Epoch 93 Avg Train Loss: 0.3819 Avg Valid Loss: 0.4659 / \n",
      "Epoch 94 Avg Train Loss: 0.3880 Avg Valid Loss: 0.4389 / \n",
      "Epoch 95 Avg Train Loss: 0.3784 Avg Valid Loss: 0.4493 / \n",
      "Epoch 96 Avg Train Loss: 0.3794 Avg Valid Loss: 0.4737 / \n",
      "Epoch 97 Avg Train Loss: 0.3842 Avg Valid Loss: 0.4774 / \n",
      "Epoch 98 Avg Train Loss: 0.3776 Avg Valid Loss: 0.4600 / \n",
      "Epoch 99 Avg Train Loss: 0.3819 Avg Valid Loss: 0.4684 / \n",
      "Epoch 100 Avg Train Loss: 0.3926 Avg Valid Loss: 0.4678 / \n",
      "========== fold: 0 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.4138\n",
      "========== stage: 1 fold: 1 training 134 / 17 ==========\n",
      "Epoch 1 Avg Train Loss: 1.4298 Avg Valid Loss: 1.2745 / \n",
      "Epoch 1 Save Best Valid Loss: 1.2745\n",
      "Epoch 2 Avg Train Loss: 1.2581 Avg Valid Loss: 1.1823 / \n",
      "Epoch 2 Save Best Valid Loss: 1.1823\n",
      "Epoch 3 Avg Train Loss: 1.1822 Avg Valid Loss: 1.1788 / \n",
      "Epoch 3 Save Best Valid Loss: 1.1788\n",
      "Epoch 4 Avg Train Loss: 1.0857 Avg Valid Loss: 1.2413 / \n",
      "Epoch 5 Avg Train Loss: 1.0343 Avg Valid Loss: 1.1065 / \n",
      "Epoch 5 Save Best Valid Loss: 1.1065\n",
      "Epoch 6 Avg Train Loss: 1.0059 Avg Valid Loss: 1.0747 / \n",
      "Epoch 6 Save Best Valid Loss: 1.0747\n",
      "Epoch 7 Avg Train Loss: 0.9813 Avg Valid Loss: 1.0160 / \n",
      "Epoch 7 Save Best Valid Loss: 1.0160\n",
      "Epoch 8 Avg Train Loss: 0.9750 Avg Valid Loss: 1.0463 / \n",
      "Epoch 9 Avg Train Loss: 0.9506 Avg Valid Loss: 1.0372 / \n",
      "Epoch 10 Avg Train Loss: 0.9473 Avg Valid Loss: 1.0208 / \n",
      "Epoch 11 Avg Train Loss: 0.9275 Avg Valid Loss: 1.0808 / \n",
      "Epoch 12 Avg Train Loss: 0.9324 Avg Valid Loss: 1.2043 / \n",
      "Epoch 13 Avg Train Loss: 0.9214 Avg Valid Loss: 0.9950 / \n",
      "Epoch 13 Save Best Valid Loss: 0.9950\n",
      "Epoch 14 Avg Train Loss: 0.9110 Avg Valid Loss: 1.0120 / \n",
      "Epoch 15 Avg Train Loss: 0.9033 Avg Valid Loss: 1.1163 / \n",
      "Epoch 16 Avg Train Loss: 0.8878 Avg Valid Loss: 1.0923 / \n",
      "Epoch 17 Avg Train Loss: 0.8884 Avg Valid Loss: 0.9937 / \n",
      "Epoch 17 Save Best Valid Loss: 0.9937\n",
      "Epoch 18 Avg Train Loss: 0.8788 Avg Valid Loss: 1.0509 / \n",
      "Epoch 19 Avg Train Loss: 0.8768 Avg Valid Loss: 1.0321 / \n",
      "Epoch 20 Avg Train Loss: 0.8720 Avg Valid Loss: 0.9889 / \n",
      "Epoch 20 Save Best Valid Loss: 0.9889\n",
      "Epoch 21 Avg Train Loss: 0.8587 Avg Valid Loss: 0.9740 / \n",
      "Epoch 21 Save Best Valid Loss: 0.9740\n",
      "Epoch 22 Avg Train Loss: 0.8535 Avg Valid Loss: 0.9818 / \n",
      "Epoch 23 Avg Train Loss: 0.8643 Avg Valid Loss: 1.0089 / \n",
      "Epoch 24 Avg Train Loss: 0.8519 Avg Valid Loss: 1.0210 / \n",
      "Epoch 25 Avg Train Loss: 0.8620 Avg Valid Loss: 1.0317 / \n",
      "Epoch 26 Avg Train Loss: 0.8457 Avg Valid Loss: 0.9485 / \n",
      "Epoch 26 Save Best Valid Loss: 0.9485\n",
      "Epoch 27 Avg Train Loss: 0.8361 Avg Valid Loss: 0.9253 / \n",
      "Epoch 27 Save Best Valid Loss: 0.9253\n",
      "Epoch 28 Avg Train Loss: 0.8396 Avg Valid Loss: 1.0053 / \n",
      "Epoch 29 Avg Train Loss: 0.8542 Avg Valid Loss: 1.1189 / \n",
      "Epoch 30 Avg Train Loss: 0.8443 Avg Valid Loss: 1.0496 / \n",
      "Epoch 31 Avg Train Loss: 0.8463 Avg Valid Loss: 0.9746 / \n",
      "Epoch 32 Avg Train Loss: 0.8389 Avg Valid Loss: 1.0932 / \n",
      "Epoch 33 Avg Train Loss: 0.8212 Avg Valid Loss: 0.9681 / \n",
      "Epoch 34 Avg Train Loss: 0.8269 Avg Valid Loss: 0.9636 / \n",
      "Epoch 35 Avg Train Loss: 0.8256 Avg Valid Loss: 0.9687 / \n",
      "Epoch 36 Avg Train Loss: 0.8273 Avg Valid Loss: 1.0349 / \n",
      "Epoch 37 Avg Train Loss: 0.8204 Avg Valid Loss: 0.9768 / \n",
      "Epoch 38 Avg Train Loss: 0.8328 Avg Valid Loss: 1.0045 / \n",
      "Epoch 39 Avg Train Loss: 0.8221 Avg Valid Loss: 0.9851 / \n",
      "Epoch 40 Avg Train Loss: 0.8182 Avg Valid Loss: 1.0047 / \n",
      "Epoch 41 Avg Train Loss: 0.8261 Avg Valid Loss: 0.9719 / \n",
      "Epoch 42 Avg Train Loss: 0.8127 Avg Valid Loss: 0.9744 / \n",
      "Epoch 43 Avg Train Loss: 0.8083 Avg Valid Loss: 0.9983 / \n",
      "Epoch 44 Avg Train Loss: 0.8269 Avg Valid Loss: 1.0193 / \n",
      "Epoch 45 Avg Train Loss: 0.8075 Avg Valid Loss: 1.0456 / \n",
      "Epoch 46 Avg Train Loss: 0.8178 Avg Valid Loss: 1.0064 / \n",
      "Epoch 47 Avg Train Loss: 0.8076 Avg Valid Loss: 0.9528 / \n",
      "Epoch 48 Avg Train Loss: 0.8124 Avg Valid Loss: 1.0199 / \n",
      "Epoch 49 Avg Train Loss: 0.8060 Avg Valid Loss: 0.9531 / \n",
      "Epoch 50 Avg Train Loss: 0.8127 Avg Valid Loss: 0.9770 / \n",
      "========== fold: 1 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 0.9253\n",
      "========== stage: 2 fold: 1 training 65 / 9 ==========\n",
      "Epoch 1 Avg Train Loss: 0.5407 Avg Valid Loss: 0.5020 / \n",
      "Epoch 1 Save Best Valid Loss: 0.5020\n",
      "Epoch 2 Avg Train Loss: 0.4979 Avg Valid Loss: 0.4937 / \n",
      "Epoch 2 Save Best Valid Loss: 0.4937\n",
      "Epoch 3 Avg Train Loss: 0.4812 Avg Valid Loss: 0.4239 / \n",
      "Epoch 3 Save Best Valid Loss: 0.4239\n",
      "Epoch 4 Avg Train Loss: 0.4793 Avg Valid Loss: 0.4442 / \n",
      "Epoch 5 Avg Train Loss: 0.4665 Avg Valid Loss: 0.4436 / \n",
      "Epoch 6 Avg Train Loss: 0.4573 Avg Valid Loss: 0.4310 / \n",
      "Epoch 7 Avg Train Loss: 0.4563 Avg Valid Loss: 0.4055 / \n",
      "Epoch 7 Save Best Valid Loss: 0.4055\n",
      "Epoch 8 Avg Train Loss: 0.4554 Avg Valid Loss: 0.4104 / \n",
      "Epoch 9 Avg Train Loss: 0.4490 Avg Valid Loss: 0.4427 / \n",
      "Epoch 10 Avg Train Loss: 0.4473 Avg Valid Loss: 0.4133 / \n",
      "Epoch 11 Avg Train Loss: 0.4520 Avg Valid Loss: 0.4112 / \n",
      "Epoch 12 Avg Train Loss: 0.4523 Avg Valid Loss: 0.4070 / \n",
      "Epoch 13 Avg Train Loss: 0.4481 Avg Valid Loss: 0.4539 / \n",
      "Epoch 14 Avg Train Loss: 0.4507 Avg Valid Loss: 0.4266 / \n",
      "Epoch 15 Avg Train Loss: 0.4395 Avg Valid Loss: 0.4220 / \n",
      "Epoch 16 Avg Train Loss: 0.4376 Avg Valid Loss: 0.4268 / \n",
      "Epoch 17 Avg Train Loss: 0.4376 Avg Valid Loss: 0.4116 / \n",
      "Epoch 18 Avg Train Loss: 0.4383 Avg Valid Loss: 0.4667 / \n",
      "Epoch 19 Avg Train Loss: 0.4437 Avg Valid Loss: 0.4031 / \n",
      "Epoch 19 Save Best Valid Loss: 0.4031\n",
      "Epoch 20 Avg Train Loss: 0.4319 Avg Valid Loss: 0.4146 / \n",
      "Epoch 21 Avg Train Loss: 0.4358 Avg Valid Loss: 0.3985 / \n",
      "Epoch 21 Save Best Valid Loss: 0.3985\n",
      "Epoch 22 Avg Train Loss: 0.4239 Avg Valid Loss: 0.4203 / \n",
      "Epoch 23 Avg Train Loss: 0.4289 Avg Valid Loss: 0.4070 / \n",
      "Epoch 24 Avg Train Loss: 0.4268 Avg Valid Loss: 0.4170 / \n",
      "Epoch 25 Avg Train Loss: 0.4327 Avg Valid Loss: 0.4694 / \n",
      "Epoch 26 Avg Train Loss: 0.4192 Avg Valid Loss: 0.4506 / \n",
      "Epoch 27 Avg Train Loss: 0.4256 Avg Valid Loss: 0.4026 / \n",
      "Epoch 28 Avg Train Loss: 0.4199 Avg Valid Loss: 0.4117 / \n",
      "Epoch 29 Avg Train Loss: 0.4227 Avg Valid Loss: 0.4281 / \n",
      "Epoch 30 Avg Train Loss: 0.4208 Avg Valid Loss: 0.4173 / \n",
      "Epoch 31 Avg Train Loss: 0.4211 Avg Valid Loss: 0.4268 / \n",
      "Epoch 32 Avg Train Loss: 0.4307 Avg Valid Loss: 0.4268 / \n",
      "Epoch 33 Avg Train Loss: 0.4177 Avg Valid Loss: 0.4640 / \n",
      "Epoch 34 Avg Train Loss: 0.4296 Avg Valid Loss: 0.4128 / \n",
      "Epoch 35 Avg Train Loss: 0.4133 Avg Valid Loss: 0.3994 / \n",
      "Epoch 36 Avg Train Loss: 0.4199 Avg Valid Loss: 0.4367 / \n",
      "Epoch 37 Avg Train Loss: 0.4232 Avg Valid Loss: 0.4440 / \n",
      "Epoch 38 Avg Train Loss: 0.4132 Avg Valid Loss: 0.4033 / \n",
      "Epoch 39 Avg Train Loss: 0.4121 Avg Valid Loss: 0.4090 / \n",
      "Epoch 40 Avg Train Loss: 0.4061 Avg Valid Loss: 0.4300 / \n",
      "Epoch 41 Avg Train Loss: 0.4052 Avg Valid Loss: 0.4129 / \n",
      "Epoch 42 Avg Train Loss: 0.4151 Avg Valid Loss: 0.4145 / \n",
      "Epoch 43 Avg Train Loss: 0.4146 Avg Valid Loss: 0.4020 / \n",
      "Epoch 44 Avg Train Loss: 0.4149 Avg Valid Loss: 0.3913 / \n",
      "Epoch 44 Save Best Valid Loss: 0.3913\n",
      "Epoch 45 Avg Train Loss: 0.4109 Avg Valid Loss: 0.4137 / \n",
      "Epoch 46 Avg Train Loss: 0.4060 Avg Valid Loss: 0.4153 / \n",
      "Epoch 47 Avg Train Loss: 0.4045 Avg Valid Loss: 0.4035 / \n",
      "Epoch 48 Avg Train Loss: 0.4095 Avg Valid Loss: 0.3995 / \n",
      "Epoch 49 Avg Train Loss: 0.4018 Avg Valid Loss: 0.4032 / \n",
      "Epoch 50 Avg Train Loss: 0.4119 Avg Valid Loss: 0.4104 / \n",
      "Epoch 51 Avg Train Loss: 0.4087 Avg Valid Loss: 0.4019 / \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52 Avg Train Loss: 0.3983 Avg Valid Loss: 0.3834 / \n",
      "Epoch 52 Save Best Valid Loss: 0.3834\n",
      "Epoch 53 Avg Train Loss: 0.4081 Avg Valid Loss: 0.4044 / \n",
      "Epoch 54 Avg Train Loss: 0.4094 Avg Valid Loss: 0.4464 / \n",
      "Epoch 55 Avg Train Loss: 0.4041 Avg Valid Loss: 0.4059 / \n",
      "Epoch 56 Avg Train Loss: 0.4125 Avg Valid Loss: 0.4310 / \n",
      "Epoch 57 Avg Train Loss: 0.4102 Avg Valid Loss: 0.3943 / \n",
      "Epoch 58 Avg Train Loss: 0.4081 Avg Valid Loss: 0.4000 / \n",
      "Epoch 59 Avg Train Loss: 0.3979 Avg Valid Loss: 0.4122 / \n",
      "Epoch 60 Avg Train Loss: 0.4014 Avg Valid Loss: 0.4039 / \n",
      "Epoch 61 Avg Train Loss: 0.4034 Avg Valid Loss: 0.3978 / \n",
      "Epoch 62 Avg Train Loss: 0.3913 Avg Valid Loss: 0.4051 / \n",
      "Epoch 63 Avg Train Loss: 0.3972 Avg Valid Loss: 0.3975 / \n",
      "Epoch 64 Avg Train Loss: 0.3967 Avg Valid Loss: 0.4221 / \n",
      "Epoch 65 Avg Train Loss: 0.4035 Avg Valid Loss: 0.4116 / \n",
      "Epoch 66 Avg Train Loss: 0.4040 Avg Valid Loss: 0.4237 / \n",
      "Epoch 67 Avg Train Loss: 0.3928 Avg Valid Loss: 0.4368 / \n",
      "Epoch 68 Avg Train Loss: 0.3967 Avg Valid Loss: 0.4021 / \n",
      "Epoch 69 Avg Train Loss: 0.3901 Avg Valid Loss: 0.4194 / \n",
      "Epoch 70 Avg Train Loss: 0.3986 Avg Valid Loss: 0.4612 / \n",
      "Epoch 71 Avg Train Loss: 0.4083 Avg Valid Loss: 0.4139 / \n",
      "Epoch 72 Avg Train Loss: 0.3902 Avg Valid Loss: 0.4106 / \n",
      "Epoch 73 Avg Train Loss: 0.3948 Avg Valid Loss: 0.4123 / \n",
      "Epoch 74 Avg Train Loss: 0.3906 Avg Valid Loss: 0.4468 / \n",
      "Epoch 75 Avg Train Loss: 0.4020 Avg Valid Loss: 0.4153 / \n",
      "Epoch 76 Avg Train Loss: 0.4003 Avg Valid Loss: 0.4250 / \n",
      "Epoch 77 Avg Train Loss: 0.4016 Avg Valid Loss: 0.4388 / \n",
      "Epoch 78 Avg Train Loss: 0.3946 Avg Valid Loss: 0.4137 / \n",
      "Epoch 79 Avg Train Loss: 0.3967 Avg Valid Loss: 0.4377 / \n",
      "Epoch 80 Avg Train Loss: 0.3922 Avg Valid Loss: 0.3846 / \n",
      "Epoch 81 Avg Train Loss: 0.3960 Avg Valid Loss: 0.3924 / \n",
      "Epoch 82 Avg Train Loss: 0.3926 Avg Valid Loss: 0.4274 / \n",
      "Epoch 83 Avg Train Loss: 0.3917 Avg Valid Loss: 0.4376 / \n",
      "Epoch 84 Avg Train Loss: 0.3931 Avg Valid Loss: 0.4174 / \n",
      "Epoch 85 Avg Train Loss: 0.3957 Avg Valid Loss: 0.4112 / \n",
      "Epoch 86 Avg Train Loss: 0.3919 Avg Valid Loss: 0.4126 / \n",
      "Epoch 87 Avg Train Loss: 0.3835 Avg Valid Loss: 0.4278 / \n",
      "Epoch 88 Avg Train Loss: 0.3806 Avg Valid Loss: 0.4327 / \n",
      "Epoch 89 Avg Train Loss: 0.3880 Avg Valid Loss: 0.4600 / \n",
      "Epoch 90 Avg Train Loss: 0.3993 Avg Valid Loss: 0.4506 / \n",
      "Epoch 91 Avg Train Loss: 0.3909 Avg Valid Loss: 0.4513 / \n",
      "Epoch 92 Avg Train Loss: 0.3789 Avg Valid Loss: 0.3888 / \n",
      "Epoch 93 Avg Train Loss: 0.3885 Avg Valid Loss: 0.4330 / \n",
      "Epoch 94 Avg Train Loss: 0.3872 Avg Valid Loss: 0.4528 / \n",
      "Epoch 95 Avg Train Loss: 0.3927 Avg Valid Loss: 0.4165 / \n",
      "Epoch 96 Avg Train Loss: 0.3830 Avg Valid Loss: 0.4065 / \n",
      "Epoch 97 Avg Train Loss: 0.3929 Avg Valid Loss: 0.4375 / \n",
      "Epoch 98 Avg Train Loss: 0.3909 Avg Valid Loss: 0.4228 / \n",
      "Epoch 99 Avg Train Loss: 0.3850 Avg Valid Loss: 0.4215 / \n",
      "Epoch 100 Avg Train Loss: 0.3789 Avg Valid Loss: 0.3968 / \n",
      "========== fold: 1 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.3834\n",
      "========== stage: 1 fold: 2 training 134 / 17 ==========\n",
      "Epoch 1 Avg Train Loss: 1.3981 Avg Valid Loss: 1.3464 / \n",
      "Epoch 1 Save Best Valid Loss: 1.3464\n",
      "Epoch 2 Avg Train Loss: 1.1973 Avg Valid Loss: 1.2058 / \n",
      "Epoch 2 Save Best Valid Loss: 1.2058\n",
      "Epoch 3 Avg Train Loss: 1.1064 Avg Valid Loss: 1.1086 / \n",
      "Epoch 3 Save Best Valid Loss: 1.1086\n",
      "Epoch 4 Avg Train Loss: 1.0387 Avg Valid Loss: 1.0183 / \n",
      "Epoch 4 Save Best Valid Loss: 1.0183\n",
      "Epoch 5 Avg Train Loss: 1.0210 Avg Valid Loss: 1.0225 / \n",
      "Epoch 6 Avg Train Loss: 0.9933 Avg Valid Loss: 1.0066 / \n",
      "Epoch 6 Save Best Valid Loss: 1.0066\n",
      "Epoch 7 Avg Train Loss: 0.9788 Avg Valid Loss: 0.9480 / \n",
      "Epoch 7 Save Best Valid Loss: 0.9480\n",
      "Epoch 8 Avg Train Loss: 0.9767 Avg Valid Loss: 1.0112 / \n",
      "Epoch 9 Avg Train Loss: 0.9528 Avg Valid Loss: 0.9600 / \n",
      "Epoch 10 Avg Train Loss: 0.9316 Avg Valid Loss: 0.8894 / \n",
      "Epoch 10 Save Best Valid Loss: 0.8894\n",
      "Epoch 11 Avg Train Loss: 0.9454 Avg Valid Loss: 0.9403 / \n",
      "Epoch 12 Avg Train Loss: 0.9258 Avg Valid Loss: 1.0322 / \n",
      "Epoch 13 Avg Train Loss: 0.9218 Avg Valid Loss: 0.8980 / \n",
      "Epoch 14 Avg Train Loss: 0.9203 Avg Valid Loss: 1.0006 / \n",
      "Epoch 15 Avg Train Loss: 0.9018 Avg Valid Loss: 0.9051 / \n",
      "Epoch 16 Avg Train Loss: 0.9046 Avg Valid Loss: 0.9220 / \n",
      "Epoch 17 Avg Train Loss: 0.9080 Avg Valid Loss: 0.9153 / \n",
      "Epoch 18 Avg Train Loss: 0.9020 Avg Valid Loss: 0.9534 / \n",
      "Epoch 19 Avg Train Loss: 0.8853 Avg Valid Loss: 0.8986 / \n",
      "Epoch 20 Avg Train Loss: 0.8800 Avg Valid Loss: 0.8817 / \n",
      "Epoch 20 Save Best Valid Loss: 0.8817\n",
      "Epoch 21 Avg Train Loss: 0.8773 Avg Valid Loss: 0.9125 / \n",
      "Epoch 22 Avg Train Loss: 0.8791 Avg Valid Loss: 0.9290 / \n",
      "Epoch 23 Avg Train Loss: 0.8648 Avg Valid Loss: 0.8908 / \n",
      "Epoch 24 Avg Train Loss: 0.8721 Avg Valid Loss: 0.9078 / \n",
      "Epoch 25 Avg Train Loss: 0.8656 Avg Valid Loss: 0.9512 / \n",
      "Epoch 26 Avg Train Loss: 0.8541 Avg Valid Loss: 0.9064 / \n",
      "Epoch 27 Avg Train Loss: 0.8523 Avg Valid Loss: 0.8438 / \n",
      "Epoch 27 Save Best Valid Loss: 0.8438\n",
      "Epoch 28 Avg Train Loss: 0.8582 Avg Valid Loss: 0.8569 / \n",
      "Epoch 29 Avg Train Loss: 0.8467 Avg Valid Loss: 0.9017 / \n",
      "Epoch 30 Avg Train Loss: 0.8407 Avg Valid Loss: 0.8978 / \n",
      "Epoch 31 Avg Train Loss: 0.8522 Avg Valid Loss: 0.8828 / \n",
      "Epoch 32 Avg Train Loss: 0.8257 Avg Valid Loss: 0.9352 / \n",
      "Epoch 33 Avg Train Loss: 0.8413 Avg Valid Loss: 0.8677 / \n",
      "Epoch 34 Avg Train Loss: 0.8350 Avg Valid Loss: 0.8899 / \n",
      "Epoch 35 Avg Train Loss: 0.8362 Avg Valid Loss: 0.9157 / \n",
      "Epoch 36 Avg Train Loss: 0.8314 Avg Valid Loss: 0.9255 / \n",
      "Epoch 37 Avg Train Loss: 0.8409 Avg Valid Loss: 0.8903 / \n",
      "Epoch 38 Avg Train Loss: 0.8249 Avg Valid Loss: 0.8889 / \n",
      "Epoch 39 Avg Train Loss: 0.8275 Avg Valid Loss: 0.8593 / \n",
      "Epoch 40 Avg Train Loss: 0.8310 Avg Valid Loss: 0.8701 / \n",
      "Epoch 41 Avg Train Loss: 0.8139 Avg Valid Loss: 0.8872 / \n",
      "Epoch 42 Avg Train Loss: 0.8057 Avg Valid Loss: 0.9221 / \n",
      "Epoch 43 Avg Train Loss: 0.8249 Avg Valid Loss: 0.8767 / \n",
      "Epoch 44 Avg Train Loss: 0.8225 Avg Valid Loss: 0.8908 / \n",
      "Epoch 45 Avg Train Loss: 0.8160 Avg Valid Loss: 0.9265 / \n",
      "Epoch 46 Avg Train Loss: 0.8205 Avg Valid Loss: 0.8738 / \n",
      "Epoch 47 Avg Train Loss: 0.8156 Avg Valid Loss: 0.9029 / \n",
      "Epoch 48 Avg Train Loss: 0.8122 Avg Valid Loss: 0.8998 / \n",
      "Epoch 49 Avg Train Loss: 0.8055 Avg Valid Loss: 0.8544 / \n",
      "Epoch 50 Avg Train Loss: 0.8039 Avg Valid Loss: 0.8687 / \n",
      "========== fold: 2 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 0.8438\n",
      "========== stage: 2 fold: 2 training 65 / 9 ==========\n",
      "Epoch 1 Avg Train Loss: 0.5330 Avg Valid Loss: 0.5708 / \n",
      "Epoch 1 Save Best Valid Loss: 0.5708\n",
      "Epoch 2 Avg Train Loss: 0.4810 Avg Valid Loss: 0.4669 / \n",
      "Epoch 2 Save Best Valid Loss: 0.4669\n",
      "Epoch 3 Avg Train Loss: 0.4734 Avg Valid Loss: 0.4616 / \n",
      "Epoch 3 Save Best Valid Loss: 0.4616\n",
      "Epoch 4 Avg Train Loss: 0.4642 Avg Valid Loss: 0.4643 / \n",
      "Epoch 5 Avg Train Loss: 0.4626 Avg Valid Loss: 0.4961 / \n",
      "Epoch 6 Avg Train Loss: 0.4482 Avg Valid Loss: 0.4650 / \n",
      "Epoch 7 Avg Train Loss: 0.4541 Avg Valid Loss: 0.4524 / \n",
      "Epoch 7 Save Best Valid Loss: 0.4524\n",
      "Epoch 8 Avg Train Loss: 0.4584 Avg Valid Loss: 0.4655 / \n",
      "Epoch 9 Avg Train Loss: 0.4423 Avg Valid Loss: 0.4355 / \n",
      "Epoch 9 Save Best Valid Loss: 0.4355\n",
      "Epoch 10 Avg Train Loss: 0.4413 Avg Valid Loss: 0.4633 / \n",
      "Epoch 11 Avg Train Loss: 0.4466 Avg Valid Loss: 0.4395 / \n",
      "Epoch 12 Avg Train Loss: 0.4347 Avg Valid Loss: 0.4380 / \n",
      "Epoch 13 Avg Train Loss: 0.4386 Avg Valid Loss: 0.4933 / \n",
      "Epoch 14 Avg Train Loss: 0.4345 Avg Valid Loss: 0.4573 / \n",
      "Epoch 15 Avg Train Loss: 0.4341 Avg Valid Loss: 0.4709 / \n",
      "Epoch 16 Avg Train Loss: 0.4363 Avg Valid Loss: 0.4535 / \n",
      "Epoch 17 Avg Train Loss: 0.4244 Avg Valid Loss: 0.4315 / \n",
      "Epoch 17 Save Best Valid Loss: 0.4315\n",
      "Epoch 18 Avg Train Loss: 0.4327 Avg Valid Loss: 0.4550 / \n",
      "Epoch 19 Avg Train Loss: 0.4294 Avg Valid Loss: 0.4269 / \n",
      "Epoch 19 Save Best Valid Loss: 0.4269\n",
      "Epoch 20 Avg Train Loss: 0.4210 Avg Valid Loss: 0.4702 / \n",
      "Epoch 21 Avg Train Loss: 0.4338 Avg Valid Loss: 0.4356 / \n",
      "Epoch 22 Avg Train Loss: 0.4229 Avg Valid Loss: 0.4920 / \n",
      "Epoch 23 Avg Train Loss: 0.4260 Avg Valid Loss: 0.4555 / \n",
      "Epoch 24 Avg Train Loss: 0.4190 Avg Valid Loss: 0.4466 / \n",
      "Epoch 25 Avg Train Loss: 0.4184 Avg Valid Loss: 0.4914 / \n",
      "Epoch 26 Avg Train Loss: 0.4155 Avg Valid Loss: 0.4360 / \n",
      "Epoch 27 Avg Train Loss: 0.4183 Avg Valid Loss: 0.4353 / \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 Avg Train Loss: 0.4201 Avg Valid Loss: 0.4479 / \n",
      "Epoch 29 Avg Train Loss: 0.4191 Avg Valid Loss: 0.4597 / \n",
      "Epoch 30 Avg Train Loss: 0.4228 Avg Valid Loss: 0.4526 / \n",
      "Epoch 31 Avg Train Loss: 0.4259 Avg Valid Loss: 0.4580 / \n",
      "Epoch 32 Avg Train Loss: 0.4214 Avg Valid Loss: 0.4335 / \n",
      "Epoch 33 Avg Train Loss: 0.4109 Avg Valid Loss: 0.4590 / \n",
      "Epoch 34 Avg Train Loss: 0.4197 Avg Valid Loss: 0.4582 / \n",
      "Epoch 35 Avg Train Loss: 0.4077 Avg Valid Loss: 0.4389 / \n",
      "Epoch 36 Avg Train Loss: 0.4173 Avg Valid Loss: 0.4342 / \n",
      "Epoch 37 Avg Train Loss: 0.4192 Avg Valid Loss: 0.5096 / \n",
      "Epoch 38 Avg Train Loss: 0.4187 Avg Valid Loss: 0.4447 / \n",
      "Epoch 39 Avg Train Loss: 0.4057 Avg Valid Loss: 0.4413 / \n",
      "Epoch 40 Avg Train Loss: 0.4186 Avg Valid Loss: 0.4458 / \n",
      "Epoch 41 Avg Train Loss: 0.4144 Avg Valid Loss: 0.4245 / \n",
      "Epoch 41 Save Best Valid Loss: 0.4245\n",
      "Epoch 42 Avg Train Loss: 0.4062 Avg Valid Loss: 0.4438 / \n",
      "Epoch 43 Avg Train Loss: 0.4101 Avg Valid Loss: 0.4586 / \n",
      "Epoch 44 Avg Train Loss: 0.4110 Avg Valid Loss: 0.4480 / \n",
      "Epoch 45 Avg Train Loss: 0.4054 Avg Valid Loss: 0.4567 / \n",
      "Epoch 46 Avg Train Loss: 0.3966 Avg Valid Loss: 0.4575 / \n",
      "Epoch 47 Avg Train Loss: 0.4139 Avg Valid Loss: 0.4492 / \n",
      "Epoch 48 Avg Train Loss: 0.4072 Avg Valid Loss: 0.4246 / \n",
      "Epoch 49 Avg Train Loss: 0.4112 Avg Valid Loss: 0.4225 / \n",
      "Epoch 49 Save Best Valid Loss: 0.4225\n",
      "Epoch 50 Avg Train Loss: 0.4020 Avg Valid Loss: 0.4423 / \n",
      "Epoch 51 Avg Train Loss: 0.4026 Avg Valid Loss: 0.4320 / \n",
      "Epoch 52 Avg Train Loss: 0.4012 Avg Valid Loss: 0.4308 / \n",
      "Epoch 53 Avg Train Loss: 0.3983 Avg Valid Loss: 0.4481 / \n",
      "Epoch 54 Avg Train Loss: 0.4019 Avg Valid Loss: 0.4498 / \n",
      "Epoch 55 Avg Train Loss: 0.3869 Avg Valid Loss: 0.4535 / \n",
      "Epoch 56 Avg Train Loss: 0.3993 Avg Valid Loss: 0.4403 / \n",
      "Epoch 57 Avg Train Loss: 0.4087 Avg Valid Loss: 0.4397 / \n",
      "Epoch 58 Avg Train Loss: 0.4009 Avg Valid Loss: 0.4431 / \n",
      "Epoch 59 Avg Train Loss: 0.3990 Avg Valid Loss: 0.4448 / \n",
      "Epoch 60 Avg Train Loss: 0.3941 Avg Valid Loss: 0.4309 / \n",
      "Epoch 61 Avg Train Loss: 0.3984 Avg Valid Loss: 0.4718 / \n",
      "Epoch 62 Avg Train Loss: 0.3967 Avg Valid Loss: 0.4289 / \n",
      "Epoch 63 Avg Train Loss: 0.4042 Avg Valid Loss: 0.4098 / \n",
      "Epoch 63 Save Best Valid Loss: 0.4098\n",
      "Epoch 64 Avg Train Loss: 0.3925 Avg Valid Loss: 0.4396 / \n",
      "Epoch 65 Avg Train Loss: 0.4052 Avg Valid Loss: 0.4197 / \n",
      "Epoch 66 Avg Train Loss: 0.3946 Avg Valid Loss: 0.4598 / \n",
      "Epoch 67 Avg Train Loss: 0.4014 Avg Valid Loss: 0.4100 / \n",
      "Epoch 68 Avg Train Loss: 0.3928 Avg Valid Loss: 0.4098 / \n",
      "Epoch 68 Save Best Valid Loss: 0.4098\n",
      "Epoch 69 Avg Train Loss: 0.3948 Avg Valid Loss: 0.4479 / \n",
      "Epoch 70 Avg Train Loss: 0.3929 Avg Valid Loss: 0.4593 / \n",
      "Epoch 71 Avg Train Loss: 0.3926 Avg Valid Loss: 0.4354 / \n",
      "Epoch 72 Avg Train Loss: 0.3965 Avg Valid Loss: 0.4493 / \n",
      "Epoch 73 Avg Train Loss: 0.3951 Avg Valid Loss: 0.4836 / \n",
      "Epoch 74 Avg Train Loss: 0.3917 Avg Valid Loss: 0.4349 / \n",
      "Epoch 75 Avg Train Loss: 0.3890 Avg Valid Loss: 0.4294 / \n",
      "Epoch 76 Avg Train Loss: 0.3935 Avg Valid Loss: 0.4556 / \n",
      "Epoch 77 Avg Train Loss: 0.3964 Avg Valid Loss: 0.4545 / \n",
      "Epoch 78 Avg Train Loss: 0.3904 Avg Valid Loss: 0.4202 / \n",
      "Epoch 79 Avg Train Loss: 0.3860 Avg Valid Loss: 0.4123 / \n",
      "Epoch 80 Avg Train Loss: 0.3946 Avg Valid Loss: 0.4440 / \n",
      "Epoch 81 Avg Train Loss: 0.3791 Avg Valid Loss: 0.4221 / \n",
      "Epoch 82 Avg Train Loss: 0.3986 Avg Valid Loss: 0.4210 / \n",
      "Epoch 83 Avg Train Loss: 0.3926 Avg Valid Loss: 0.4252 / \n",
      "Epoch 84 Avg Train Loss: 0.3900 Avg Valid Loss: 0.4276 / \n",
      "Epoch 85 Avg Train Loss: 0.3816 Avg Valid Loss: 0.4151 / \n",
      "Epoch 86 Avg Train Loss: 0.3842 Avg Valid Loss: 0.4519 / \n",
      "Epoch 87 Avg Train Loss: 0.3786 Avg Valid Loss: 0.4086 / \n",
      "Epoch 87 Save Best Valid Loss: 0.4086\n",
      "Epoch 88 Avg Train Loss: 0.3962 Avg Valid Loss: 0.4311 / \n",
      "Epoch 89 Avg Train Loss: 0.3912 Avg Valid Loss: 0.4484 / \n",
      "Epoch 90 Avg Train Loss: 0.3806 Avg Valid Loss: 0.4466 / \n",
      "Epoch 91 Avg Train Loss: 0.3776 Avg Valid Loss: 0.4464 / \n",
      "Epoch 92 Avg Train Loss: 0.3829 Avg Valid Loss: 0.4070 / \n",
      "Epoch 92 Save Best Valid Loss: 0.4070\n",
      "Epoch 93 Avg Train Loss: 0.3825 Avg Valid Loss: 0.4788 / \n",
      "Epoch 94 Avg Train Loss: 0.3931 Avg Valid Loss: 0.4297 / \n",
      "Epoch 95 Avg Train Loss: 0.3924 Avg Valid Loss: 0.4267 / \n",
      "Epoch 96 Avg Train Loss: 0.3809 Avg Valid Loss: 0.4264 / \n",
      "Epoch 97 Avg Train Loss: 0.3864 Avg Valid Loss: 0.4295 / \n",
      "Epoch 98 Avg Train Loss: 0.3827 Avg Valid Loss: 0.4190 / \n",
      "Epoch 99 Avg Train Loss: 0.3782 Avg Valid Loss: 0.4094 / \n",
      "Epoch 100 Avg Train Loss: 0.3947 Avg Valid Loss: 0.4832 / \n",
      "========== fold: 2 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.4070\n",
      "========== stage: 1 fold: 3 training 134 / 17 ==========\n",
      "Epoch 1 Avg Train Loss: 1.3941 Avg Valid Loss: 1.3063 / \n",
      "Epoch 1 Save Best Valid Loss: 1.3063\n",
      "Epoch 2 Avg Train Loss: 1.2019 Avg Valid Loss: 1.2659 / \n",
      "Epoch 2 Save Best Valid Loss: 1.2659\n",
      "Epoch 3 Avg Train Loss: 1.0956 Avg Valid Loss: 1.5581 / \n",
      "Epoch 4 Avg Train Loss: 1.0148 Avg Valid Loss: 1.3177 / \n",
      "Epoch 5 Avg Train Loss: 0.9906 Avg Valid Loss: 1.3152 / \n",
      "Epoch 6 Avg Train Loss: 0.9725 Avg Valid Loss: 1.3266 / \n",
      "Epoch 7 Avg Train Loss: 0.9506 Avg Valid Loss: 1.2564 / \n",
      "Epoch 7 Save Best Valid Loss: 1.2564\n",
      "Epoch 8 Avg Train Loss: 0.9380 Avg Valid Loss: 1.2453 / \n",
      "Epoch 8 Save Best Valid Loss: 1.2453\n",
      "Epoch 9 Avg Train Loss: 0.9215 Avg Valid Loss: 1.2050 / \n",
      "Epoch 9 Save Best Valid Loss: 1.2050\n",
      "Epoch 10 Avg Train Loss: 0.9272 Avg Valid Loss: 1.1352 / \n",
      "Epoch 10 Save Best Valid Loss: 1.1352\n",
      "Epoch 11 Avg Train Loss: 0.9078 Avg Valid Loss: 1.2740 / \n",
      "Epoch 12 Avg Train Loss: 0.9063 Avg Valid Loss: 1.2383 / \n",
      "Epoch 13 Avg Train Loss: 0.8905 Avg Valid Loss: 1.1726 / \n",
      "Epoch 14 Avg Train Loss: 0.8791 Avg Valid Loss: 1.2060 / \n",
      "Epoch 15 Avg Train Loss: 0.8782 Avg Valid Loss: 1.2643 / \n",
      "Epoch 16 Avg Train Loss: 0.8793 Avg Valid Loss: 1.1484 / \n",
      "Epoch 17 Avg Train Loss: 0.8727 Avg Valid Loss: 1.1572 / \n",
      "Epoch 18 Avg Train Loss: 0.8663 Avg Valid Loss: 1.2075 / \n",
      "Epoch 19 Avg Train Loss: 0.8693 Avg Valid Loss: 1.2081 / \n",
      "Epoch 20 Avg Train Loss: 0.8708 Avg Valid Loss: 1.1231 / \n",
      "Epoch 20 Save Best Valid Loss: 1.1231\n",
      "Epoch 21 Avg Train Loss: 0.8561 Avg Valid Loss: 1.1428 / \n",
      "Epoch 22 Avg Train Loss: 0.8513 Avg Valid Loss: 1.1705 / \n",
      "Epoch 23 Avg Train Loss: 0.8523 Avg Valid Loss: 1.1086 / \n",
      "Epoch 23 Save Best Valid Loss: 1.1086\n",
      "Epoch 24 Avg Train Loss: 0.8480 Avg Valid Loss: 1.1854 / \n",
      "Epoch 25 Avg Train Loss: 0.8404 Avg Valid Loss: 1.2271 / \n",
      "Epoch 26 Avg Train Loss: 0.8381 Avg Valid Loss: 1.1768 / \n",
      "Epoch 27 Avg Train Loss: 0.8361 Avg Valid Loss: 1.1480 / \n",
      "Epoch 28 Avg Train Loss: 0.8395 Avg Valid Loss: 1.1799 / \n",
      "Epoch 29 Avg Train Loss: 0.8417 Avg Valid Loss: 1.2078 / \n",
      "Epoch 30 Avg Train Loss: 0.8209 Avg Valid Loss: 1.2004 / \n",
      "Epoch 31 Avg Train Loss: 0.8213 Avg Valid Loss: 1.2179 / \n",
      "Epoch 32 Avg Train Loss: 0.8351 Avg Valid Loss: 1.1978 / \n",
      "Epoch 33 Avg Train Loss: 0.8185 Avg Valid Loss: 1.1978 / \n",
      "Epoch 34 Avg Train Loss: 0.8280 Avg Valid Loss: 1.2156 / \n",
      "Epoch 35 Avg Train Loss: 0.8227 Avg Valid Loss: 1.1435 / \n",
      "Epoch 36 Avg Train Loss: 0.8162 Avg Valid Loss: 1.1873 / \n",
      "Epoch 37 Avg Train Loss: 0.8245 Avg Valid Loss: 1.1595 / \n",
      "Epoch 38 Avg Train Loss: 0.8205 Avg Valid Loss: 1.1372 / \n",
      "Epoch 39 Avg Train Loss: 0.8070 Avg Valid Loss: 1.1403 / \n",
      "Epoch 40 Avg Train Loss: 0.7977 Avg Valid Loss: 1.1097 / \n",
      "Epoch 41 Avg Train Loss: 0.8082 Avg Valid Loss: 1.1452 / \n",
      "Epoch 42 Avg Train Loss: 0.8023 Avg Valid Loss: 1.2033 / \n",
      "Epoch 43 Avg Train Loss: 0.8005 Avg Valid Loss: 1.1237 / \n",
      "Epoch 44 Avg Train Loss: 0.7930 Avg Valid Loss: 1.1439 / \n",
      "Epoch 45 Avg Train Loss: 0.8029 Avg Valid Loss: 1.1358 / \n",
      "Epoch 46 Avg Train Loss: 0.7923 Avg Valid Loss: 1.1526 / \n",
      "Epoch 47 Avg Train Loss: 0.8025 Avg Valid Loss: 1.1301 / \n",
      "Epoch 48 Avg Train Loss: 0.7922 Avg Valid Loss: 1.0842 / \n",
      "Epoch 48 Save Best Valid Loss: 1.0842\n",
      "Epoch 49 Avg Train Loss: 0.7950 Avg Valid Loss: 1.1718 / \n",
      "Epoch 50 Avg Train Loss: 0.7972 Avg Valid Loss: 1.1528 / \n",
      "========== fold: 3 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 1.0842\n",
      "========== stage: 2 fold: 3 training 65 / 9 ==========\n",
      "Epoch 1 Avg Train Loss: 0.5251 Avg Valid Loss: 0.4585 / \n",
      "Epoch 1 Save Best Valid Loss: 0.4585\n",
      "Epoch 2 Avg Train Loss: 0.4745 Avg Valid Loss: 0.4547 / \n",
      "Epoch 2 Save Best Valid Loss: 0.4547\n",
      "Epoch 3 Avg Train Loss: 0.4722 Avg Valid Loss: 0.4373 / \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Save Best Valid Loss: 0.4373\n",
      "Epoch 4 Avg Train Loss: 0.4522 Avg Valid Loss: 0.4113 / \n",
      "Epoch 4 Save Best Valid Loss: 0.4113\n",
      "Epoch 5 Avg Train Loss: 0.4476 Avg Valid Loss: 0.4368 / \n",
      "Epoch 6 Avg Train Loss: 0.4454 Avg Valid Loss: 0.4453 / \n",
      "Epoch 7 Avg Train Loss: 0.4397 Avg Valid Loss: 0.4275 / \n",
      "Epoch 8 Avg Train Loss: 0.4370 Avg Valid Loss: 0.4284 / \n",
      "Epoch 9 Avg Train Loss: 0.4341 Avg Valid Loss: 0.4415 / \n",
      "Epoch 10 Avg Train Loss: 0.4435 Avg Valid Loss: 0.4399 / \n",
      "Epoch 11 Avg Train Loss: 0.4335 Avg Valid Loss: 0.4163 / \n",
      "Epoch 12 Avg Train Loss: 0.4351 Avg Valid Loss: 0.4346 / \n",
      "Epoch 13 Avg Train Loss: 0.4281 Avg Valid Loss: 0.4379 / \n",
      "Epoch 14 Avg Train Loss: 0.4209 Avg Valid Loss: 0.4467 / \n",
      "Epoch 15 Avg Train Loss: 0.4299 Avg Valid Loss: 0.4425 / \n",
      "Epoch 16 Avg Train Loss: 0.4320 Avg Valid Loss: 0.4299 / \n",
      "Epoch 17 Avg Train Loss: 0.4249 Avg Valid Loss: 0.5099 / \n",
      "Epoch 18 Avg Train Loss: 0.4225 Avg Valid Loss: 0.4567 / \n",
      "Epoch 19 Avg Train Loss: 0.4233 Avg Valid Loss: 0.4336 / \n",
      "Epoch 20 Avg Train Loss: 0.4161 Avg Valid Loss: 0.4337 / \n",
      "Epoch 21 Avg Train Loss: 0.4172 Avg Valid Loss: 0.4576 / \n",
      "Epoch 22 Avg Train Loss: 0.4172 Avg Valid Loss: 0.4726 / \n",
      "Epoch 23 Avg Train Loss: 0.4219 Avg Valid Loss: 0.4507 / \n",
      "Epoch 24 Avg Train Loss: 0.4177 Avg Valid Loss: 0.4575 / \n",
      "Epoch 25 Avg Train Loss: 0.4193 Avg Valid Loss: 0.4260 / \n",
      "Epoch 26 Avg Train Loss: 0.4130 Avg Valid Loss: 0.4433 / \n",
      "Epoch 27 Avg Train Loss: 0.4041 Avg Valid Loss: 0.4501 / \n",
      "Epoch 28 Avg Train Loss: 0.4108 Avg Valid Loss: 0.4447 / \n",
      "Epoch 29 Avg Train Loss: 0.4085 Avg Valid Loss: 0.4317 / \n",
      "Epoch 30 Avg Train Loss: 0.4125 Avg Valid Loss: 0.4630 / \n",
      "Epoch 31 Avg Train Loss: 0.4121 Avg Valid Loss: 0.4341 / \n",
      "Epoch 32 Avg Train Loss: 0.4138 Avg Valid Loss: 0.4470 / \n",
      "Epoch 33 Avg Train Loss: 0.4022 Avg Valid Loss: 0.4477 / \n",
      "Epoch 34 Avg Train Loss: 0.4054 Avg Valid Loss: 0.4356 / \n",
      "Epoch 35 Avg Train Loss: 0.4023 Avg Valid Loss: 0.4266 / \n",
      "Epoch 36 Avg Train Loss: 0.4053 Avg Valid Loss: 0.4412 / \n",
      "Epoch 37 Avg Train Loss: 0.4095 Avg Valid Loss: 0.4388 / \n",
      "Epoch 38 Avg Train Loss: 0.4010 Avg Valid Loss: 0.4215 / \n",
      "Epoch 39 Avg Train Loss: 0.4036 Avg Valid Loss: 0.4378 / \n",
      "Epoch 40 Avg Train Loss: 0.4098 Avg Valid Loss: 0.4152 / \n",
      "Epoch 41 Avg Train Loss: 0.4158 Avg Valid Loss: 0.4724 / \n",
      "Epoch 42 Avg Train Loss: 0.4113 Avg Valid Loss: 0.4427 / \n",
      "Epoch 43 Avg Train Loss: 0.4008 Avg Valid Loss: 0.4536 / \n",
      "Epoch 44 Avg Train Loss: 0.4034 Avg Valid Loss: 0.4607 / \n",
      "Epoch 45 Avg Train Loss: 0.3941 Avg Valid Loss: 0.4486 / \n",
      "Epoch 46 Avg Train Loss: 0.4030 Avg Valid Loss: 0.4225 / \n",
      "Epoch 47 Avg Train Loss: 0.4071 Avg Valid Loss: 0.4305 / \n",
      "Epoch 48 Avg Train Loss: 0.4022 Avg Valid Loss: 0.4363 / \n",
      "Epoch 49 Avg Train Loss: 0.3918 Avg Valid Loss: 0.4312 / \n",
      "Epoch 50 Avg Train Loss: 0.4039 Avg Valid Loss: 0.4322 / \n",
      "Epoch 51 Avg Train Loss: 0.3991 Avg Valid Loss: 0.4321 / \n",
      "Epoch 52 Avg Train Loss: 0.3971 Avg Valid Loss: 0.4355 / \n",
      "Epoch 53 Avg Train Loss: 0.3961 Avg Valid Loss: 0.4209 / \n",
      "Epoch 54 Avg Train Loss: 0.4011 Avg Valid Loss: 0.4692 / \n",
      "Epoch 55 Avg Train Loss: 0.4022 Avg Valid Loss: 0.4262 / \n",
      "Epoch 56 Avg Train Loss: 0.3981 Avg Valid Loss: 0.4360 / \n",
      "Epoch 57 Avg Train Loss: 0.3921 Avg Valid Loss: 0.4434 / \n",
      "Epoch 58 Avg Train Loss: 0.3960 Avg Valid Loss: 0.4501 / \n",
      "Epoch 59 Avg Train Loss: 0.3969 Avg Valid Loss: 0.4437 / \n",
      "Epoch 60 Avg Train Loss: 0.4017 Avg Valid Loss: 0.4337 / \n",
      "Epoch 61 Avg Train Loss: 0.3911 Avg Valid Loss: 0.4306 / \n",
      "Epoch 62 Avg Train Loss: 0.3916 Avg Valid Loss: 0.4436 / \n",
      "Epoch 63 Avg Train Loss: 0.3956 Avg Valid Loss: 0.4243 / \n",
      "Epoch 64 Avg Train Loss: 0.3954 Avg Valid Loss: 0.4442 / \n",
      "Epoch 65 Avg Train Loss: 0.3965 Avg Valid Loss: 0.4170 / \n",
      "Epoch 66 Avg Train Loss: 0.3974 Avg Valid Loss: 0.4527 / \n",
      "Epoch 67 Avg Train Loss: 0.3919 Avg Valid Loss: 0.4477 / \n",
      "Epoch 68 Avg Train Loss: 0.3956 Avg Valid Loss: 0.4330 / \n",
      "Epoch 69 Avg Train Loss: 0.3952 Avg Valid Loss: 0.4438 / \n",
      "Epoch 70 Avg Train Loss: 0.3915 Avg Valid Loss: 0.4757 / \n",
      "Epoch 71 Avg Train Loss: 0.3896 Avg Valid Loss: 0.4403 / \n",
      "Epoch 72 Avg Train Loss: 0.3805 Avg Valid Loss: 0.4346 / \n",
      "Epoch 73 Avg Train Loss: 0.3944 Avg Valid Loss: 0.4603 / \n",
      "Epoch 74 Avg Train Loss: 0.3912 Avg Valid Loss: 0.4203 / \n",
      "Epoch 75 Avg Train Loss: 0.3861 Avg Valid Loss: 0.4359 / \n",
      "Epoch 76 Avg Train Loss: 0.3866 Avg Valid Loss: 0.4227 / \n",
      "Epoch 77 Avg Train Loss: 0.3874 Avg Valid Loss: 0.4387 / \n",
      "Epoch 78 Avg Train Loss: 0.3860 Avg Valid Loss: 0.4500 / \n",
      "Epoch 79 Avg Train Loss: 0.3825 Avg Valid Loss: 0.4540 / \n",
      "Epoch 80 Avg Train Loss: 0.3792 Avg Valid Loss: 0.4357 / \n",
      "Epoch 81 Avg Train Loss: 0.3912 Avg Valid Loss: 0.4306 / \n",
      "Epoch 82 Avg Train Loss: 0.3899 Avg Valid Loss: 0.4706 / \n",
      "Epoch 83 Avg Train Loss: 0.3820 Avg Valid Loss: 0.4327 / \n",
      "Epoch 84 Avg Train Loss: 0.3795 Avg Valid Loss: 0.4327 / \n",
      "Epoch 85 Avg Train Loss: 0.3807 Avg Valid Loss: 0.4315 / \n",
      "Epoch 86 Avg Train Loss: 0.3887 Avg Valid Loss: 0.4265 / \n",
      "Epoch 87 Avg Train Loss: 0.3727 Avg Valid Loss: 0.4354 / \n",
      "Epoch 88 Avg Train Loss: 0.3808 Avg Valid Loss: 0.4111 / \n",
      "Epoch 88 Save Best Valid Loss: 0.4111\n",
      "Epoch 89 Avg Train Loss: 0.3814 Avg Valid Loss: 0.4192 / \n",
      "Epoch 90 Avg Train Loss: 0.3893 Avg Valid Loss: 0.4439 / \n",
      "Epoch 91 Avg Train Loss: 0.3860 Avg Valid Loss: 0.4202 / \n",
      "Epoch 92 Avg Train Loss: 0.3757 Avg Valid Loss: 0.4319 / \n",
      "Epoch 93 Avg Train Loss: 0.3817 Avg Valid Loss: 0.4133 / \n",
      "Epoch 94 Avg Train Loss: 0.3794 Avg Valid Loss: 0.4172 / \n",
      "Epoch 95 Avg Train Loss: 0.3892 Avg Valid Loss: 0.4541 / \n",
      "Epoch 96 Avg Train Loss: 0.3765 Avg Valid Loss: 0.4232 / \n",
      "Epoch 97 Avg Train Loss: 0.3805 Avg Valid Loss: 0.4290 / \n",
      "Epoch 98 Avg Train Loss: 0.3853 Avg Valid Loss: 0.4520 / \n",
      "Epoch 99 Avg Train Loss: 0.3789 Avg Valid Loss: 0.4249 / \n",
      "Epoch 100 Avg Train Loss: 0.3784 Avg Valid Loss: 0.4229 / \n",
      "========== fold: 3 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.4111\n",
      "========== stage: 1 fold: 4 training 134 / 17 ==========\n",
      "Epoch 1 Avg Train Loss: 1.4133 Avg Valid Loss: 1.4151 / \n",
      "Epoch 1 Save Best Valid Loss: 1.4151\n",
      "Epoch 2 Avg Train Loss: 1.2502 Avg Valid Loss: 1.1886 / \n",
      "Epoch 2 Save Best Valid Loss: 1.1886\n",
      "Epoch 3 Avg Train Loss: 1.1641 Avg Valid Loss: 1.1232 / \n",
      "Epoch 3 Save Best Valid Loss: 1.1232\n",
      "Epoch 4 Avg Train Loss: 1.0828 Avg Valid Loss: 1.0835 / \n",
      "Epoch 4 Save Best Valid Loss: 1.0835\n",
      "Epoch 5 Avg Train Loss: 1.0437 Avg Valid Loss: 1.1812 / \n",
      "Epoch 6 Avg Train Loss: 1.0086 Avg Valid Loss: 1.0575 / \n",
      "Epoch 6 Save Best Valid Loss: 1.0575\n",
      "Epoch 7 Avg Train Loss: 0.9922 Avg Valid Loss: 0.9770 / \n",
      "Epoch 7 Save Best Valid Loss: 0.9770\n",
      "Epoch 8 Avg Train Loss: 0.9713 Avg Valid Loss: 1.0638 / \n",
      "Epoch 9 Avg Train Loss: 0.9741 Avg Valid Loss: 1.0958 / \n",
      "Epoch 10 Avg Train Loss: 0.9649 Avg Valid Loss: 1.0730 / \n",
      "Epoch 11 Avg Train Loss: 0.9610 Avg Valid Loss: 0.9502 / \n",
      "Epoch 11 Save Best Valid Loss: 0.9502\n",
      "Epoch 12 Avg Train Loss: 0.9428 Avg Valid Loss: 1.0271 / \n",
      "Epoch 13 Avg Train Loss: 0.9296 Avg Valid Loss: 0.9690 / \n",
      "Epoch 14 Avg Train Loss: 0.9362 Avg Valid Loss: 0.9243 / \n",
      "Epoch 14 Save Best Valid Loss: 0.9243\n",
      "Epoch 15 Avg Train Loss: 0.9031 Avg Valid Loss: 0.9356 / \n",
      "Epoch 16 Avg Train Loss: 0.9008 Avg Valid Loss: 1.0036 / \n",
      "Epoch 17 Avg Train Loss: 0.8985 Avg Valid Loss: 0.9467 / \n",
      "Epoch 18 Avg Train Loss: 0.8898 Avg Valid Loss: 0.9049 / \n",
      "Epoch 18 Save Best Valid Loss: 0.9049\n",
      "Epoch 19 Avg Train Loss: 0.8813 Avg Valid Loss: 0.9426 / \n",
      "Epoch 20 Avg Train Loss: 0.8745 Avg Valid Loss: 0.9042 / \n",
      "Epoch 20 Save Best Valid Loss: 0.9042\n",
      "Epoch 21 Avg Train Loss: 0.8684 Avg Valid Loss: 0.9051 / \n",
      "Epoch 22 Avg Train Loss: 0.8692 Avg Valid Loss: 1.0172 / \n",
      "Epoch 23 Avg Train Loss: 0.8628 Avg Valid Loss: 0.8935 / \n",
      "Epoch 23 Save Best Valid Loss: 0.8935\n",
      "Epoch 24 Avg Train Loss: 0.8716 Avg Valid Loss: 0.9031 / \n",
      "Epoch 25 Avg Train Loss: 0.8637 Avg Valid Loss: 1.0092 / \n",
      "Epoch 26 Avg Train Loss: 0.8680 Avg Valid Loss: 0.9321 / \n",
      "Epoch 27 Avg Train Loss: 0.8597 Avg Valid Loss: 0.8696 / \n",
      "Epoch 27 Save Best Valid Loss: 0.8696\n",
      "Epoch 28 Avg Train Loss: 0.8574 Avg Valid Loss: 0.9285 / \n",
      "Epoch 29 Avg Train Loss: 0.8580 Avg Valid Loss: 0.9392 / \n",
      "Epoch 30 Avg Train Loss: 0.8423 Avg Valid Loss: 0.9041 / \n",
      "Epoch 31 Avg Train Loss: 0.8433 Avg Valid Loss: 0.9762 / \n",
      "Epoch 32 Avg Train Loss: 0.8275 Avg Valid Loss: 0.9298 / \n",
      "Epoch 33 Avg Train Loss: 0.8389 Avg Valid Loss: 0.8719 / \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34 Avg Train Loss: 0.8543 Avg Valid Loss: 0.8943 / \n",
      "Epoch 35 Avg Train Loss: 0.8379 Avg Valid Loss: 0.8956 / \n",
      "Epoch 36 Avg Train Loss: 0.8257 Avg Valid Loss: 0.8944 / \n",
      "Epoch 37 Avg Train Loss: 0.8395 Avg Valid Loss: 0.9004 / \n",
      "Epoch 38 Avg Train Loss: 0.8357 Avg Valid Loss: 0.8912 / \n",
      "Epoch 39 Avg Train Loss: 0.8241 Avg Valid Loss: 0.9175 / \n",
      "Epoch 40 Avg Train Loss: 0.8229 Avg Valid Loss: 0.9161 / \n",
      "Epoch 41 Avg Train Loss: 0.8123 Avg Valid Loss: 0.8792 / \n",
      "Epoch 42 Avg Train Loss: 0.8242 Avg Valid Loss: 0.9056 / \n",
      "Epoch 43 Avg Train Loss: 0.8309 Avg Valid Loss: 0.9220 / \n",
      "Epoch 44 Avg Train Loss: 0.8041 Avg Valid Loss: 0.9657 / \n",
      "Epoch 45 Avg Train Loss: 0.8215 Avg Valid Loss: 0.9270 / \n",
      "Epoch 46 Avg Train Loss: 0.8153 Avg Valid Loss: 0.9187 / \n",
      "Epoch 47 Avg Train Loss: 0.8059 Avg Valid Loss: 0.8566 / \n",
      "Epoch 47 Save Best Valid Loss: 0.8566\n",
      "Epoch 48 Avg Train Loss: 0.8040 Avg Valid Loss: 0.8789 / \n",
      "Epoch 49 Avg Train Loss: 0.8085 Avg Valid Loss: 0.8904 / \n",
      "Epoch 50 Avg Train Loss: 0.8270 Avg Valid Loss: 0.8669 / \n",
      "========== fold: 4 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 0.8566\n",
      "========== stage: 2 fold: 4 training 65 / 9 ==========\n",
      "Epoch 1 Avg Train Loss: 0.5118 Avg Valid Loss: 0.4897 / \n",
      "Epoch 1 Save Best Valid Loss: 0.4897\n",
      "Epoch 2 Avg Train Loss: 0.4677 Avg Valid Loss: 0.4766 / \n",
      "Epoch 2 Save Best Valid Loss: 0.4766\n",
      "Epoch 3 Avg Train Loss: 0.4609 Avg Valid Loss: 0.4390 / \n",
      "Epoch 3 Save Best Valid Loss: 0.4390\n",
      "Epoch 4 Avg Train Loss: 0.4440 Avg Valid Loss: 0.4422 / \n",
      "Epoch 5 Avg Train Loss: 0.4530 Avg Valid Loss: 0.4350 / \n",
      "Epoch 5 Save Best Valid Loss: 0.4350\n",
      "Epoch 6 Avg Train Loss: 0.4357 Avg Valid Loss: 0.4321 / \n",
      "Epoch 6 Save Best Valid Loss: 0.4321\n",
      "Epoch 7 Avg Train Loss: 0.4317 Avg Valid Loss: 0.4305 / \n",
      "Epoch 7 Save Best Valid Loss: 0.4305\n",
      "Epoch 8 Avg Train Loss: 0.4305 Avg Valid Loss: 0.4593 / \n",
      "Epoch 9 Avg Train Loss: 0.4276 Avg Valid Loss: 0.4920 / \n",
      "Epoch 10 Avg Train Loss: 0.4174 Avg Valid Loss: 0.4285 / \n",
      "Epoch 10 Save Best Valid Loss: 0.4285\n",
      "Epoch 11 Avg Train Loss: 0.4199 Avg Valid Loss: 0.4558 / \n",
      "Epoch 12 Avg Train Loss: 0.4173 Avg Valid Loss: 0.4673 / \n",
      "Epoch 13 Avg Train Loss: 0.4196 Avg Valid Loss: 0.4429 / \n",
      "Epoch 14 Avg Train Loss: 0.4060 Avg Valid Loss: 0.4608 / \n",
      "Epoch 15 Avg Train Loss: 0.4117 Avg Valid Loss: 0.4676 / \n",
      "Epoch 16 Avg Train Loss: 0.4196 Avg Valid Loss: 0.4458 / \n",
      "Epoch 17 Avg Train Loss: 0.4109 Avg Valid Loss: 0.4475 / \n",
      "Epoch 18 Avg Train Loss: 0.4135 Avg Valid Loss: 0.4656 / \n",
      "Epoch 19 Avg Train Loss: 0.4139 Avg Valid Loss: 0.4406 / \n",
      "Epoch 20 Avg Train Loss: 0.4140 Avg Valid Loss: 0.4427 / \n",
      "Epoch 21 Avg Train Loss: 0.4055 Avg Valid Loss: 0.4777 / \n",
      "Epoch 22 Avg Train Loss: 0.4137 Avg Valid Loss: 0.4257 / \n",
      "Epoch 22 Save Best Valid Loss: 0.4257\n",
      "Epoch 23 Avg Train Loss: 0.4176 Avg Valid Loss: 0.4708 / \n",
      "Epoch 24 Avg Train Loss: 0.4012 Avg Valid Loss: 0.4383 / \n",
      "Epoch 25 Avg Train Loss: 0.4018 Avg Valid Loss: 0.4512 / \n",
      "Epoch 26 Avg Train Loss: 0.4014 Avg Valid Loss: 0.4635 / \n",
      "Epoch 27 Avg Train Loss: 0.3971 Avg Valid Loss: 0.4718 / \n",
      "Epoch 28 Avg Train Loss: 0.4071 Avg Valid Loss: 0.4362 / \n",
      "Epoch 29 Avg Train Loss: 0.3958 Avg Valid Loss: 0.4918 / \n",
      "Epoch 30 Avg Train Loss: 0.4085 Avg Valid Loss: 0.4648 / \n",
      "Epoch 31 Avg Train Loss: 0.3864 Avg Valid Loss: 0.4660 / \n",
      "Epoch 32 Avg Train Loss: 0.3908 Avg Valid Loss: 0.4589 / \n",
      "Epoch 33 Avg Train Loss: 0.4011 Avg Valid Loss: 0.4319 / \n",
      "Epoch 34 Avg Train Loss: 0.3916 Avg Valid Loss: 0.4616 / \n",
      "Epoch 35 Avg Train Loss: 0.3956 Avg Valid Loss: 0.4487 / \n",
      "Epoch 36 Avg Train Loss: 0.3954 Avg Valid Loss: 0.4561 / \n",
      "Epoch 37 Avg Train Loss: 0.3958 Avg Valid Loss: 0.4439 / \n",
      "Epoch 38 Avg Train Loss: 0.3981 Avg Valid Loss: 0.4440 / \n",
      "Epoch 39 Avg Train Loss: 0.3894 Avg Valid Loss: 0.4226 / \n",
      "Epoch 39 Save Best Valid Loss: 0.4226\n",
      "Epoch 40 Avg Train Loss: 0.3877 Avg Valid Loss: 0.4262 / \n",
      "Epoch 41 Avg Train Loss: 0.3879 Avg Valid Loss: 0.4766 / \n",
      "Epoch 42 Avg Train Loss: 0.3877 Avg Valid Loss: 0.4547 / \n",
      "Epoch 43 Avg Train Loss: 0.3798 Avg Valid Loss: 0.4762 / \n",
      "Epoch 44 Avg Train Loss: 0.3945 Avg Valid Loss: 0.4363 / \n",
      "Epoch 45 Avg Train Loss: 0.3937 Avg Valid Loss: 0.4623 / \n",
      "Epoch 46 Avg Train Loss: 0.3905 Avg Valid Loss: 0.4626 / \n",
      "Epoch 47 Avg Train Loss: 0.4043 Avg Valid Loss: 0.4746 / \n",
      "Epoch 48 Avg Train Loss: 0.3904 Avg Valid Loss: 0.4347 / \n",
      "Epoch 49 Avg Train Loss: 0.3860 Avg Valid Loss: 0.4504 / \n",
      "Epoch 50 Avg Train Loss: 0.3919 Avg Valid Loss: 0.4596 / \n",
      "Epoch 51 Avg Train Loss: 0.3924 Avg Valid Loss: 0.4791 / \n",
      "Epoch 52 Avg Train Loss: 0.3845 Avg Valid Loss: 0.4365 / \n",
      "Epoch 53 Avg Train Loss: 0.3943 Avg Valid Loss: 0.4835 / \n",
      "Epoch 54 Avg Train Loss: 0.3899 Avg Valid Loss: 0.4248 / \n",
      "Epoch 55 Avg Train Loss: 0.3774 Avg Valid Loss: 0.4480 / \n",
      "Epoch 56 Avg Train Loss: 0.3827 Avg Valid Loss: 0.4375 / \n",
      "Epoch 57 Avg Train Loss: 0.3811 Avg Valid Loss: 0.4494 / \n",
      "Epoch 58 Avg Train Loss: 0.3809 Avg Valid Loss: 0.4545 / \n",
      "Epoch 59 Avg Train Loss: 0.3872 Avg Valid Loss: 0.4350 / \n",
      "Epoch 60 Avg Train Loss: 0.3846 Avg Valid Loss: 0.4660 / \n",
      "Epoch 61 Avg Train Loss: 0.3839 Avg Valid Loss: 0.5086 / \n",
      "Epoch 62 Avg Train Loss: 0.3894 Avg Valid Loss: 0.4870 / \n",
      "Epoch 63 Avg Train Loss: 0.3847 Avg Valid Loss: 0.4492 / \n",
      "Epoch 64 Avg Train Loss: 0.3837 Avg Valid Loss: 0.4660 / \n",
      "Epoch 65 Avg Train Loss: 0.3794 Avg Valid Loss: 0.4864 / \n",
      "Epoch 66 Avg Train Loss: 0.3737 Avg Valid Loss: 0.4565 / \n",
      "Epoch 67 Avg Train Loss: 0.3864 Avg Valid Loss: 0.4324 / \n",
      "Epoch 68 Avg Train Loss: 0.3793 Avg Valid Loss: 0.4471 / \n",
      "Epoch 69 Avg Train Loss: 0.3806 Avg Valid Loss: 0.4993 / \n",
      "Epoch 70 Avg Train Loss: 0.3884 Avg Valid Loss: 0.4583 / \n",
      "Epoch 71 Avg Train Loss: 0.3908 Avg Valid Loss: 0.4449 / \n",
      "Epoch 72 Avg Train Loss: 0.3766 Avg Valid Loss: 0.4531 / \n",
      "Epoch 73 Avg Train Loss: 0.3721 Avg Valid Loss: 0.4382 / \n",
      "Epoch 74 Avg Train Loss: 0.3742 Avg Valid Loss: 0.4810 / \n",
      "Epoch 75 Avg Train Loss: 0.3729 Avg Valid Loss: 0.4537 / \n",
      "Epoch 76 Avg Train Loss: 0.3712 Avg Valid Loss: 0.4450 / \n",
      "Epoch 77 Avg Train Loss: 0.3816 Avg Valid Loss: 0.4749 / \n",
      "Epoch 78 Avg Train Loss: 0.3766 Avg Valid Loss: 0.4734 / \n",
      "Epoch 79 Avg Train Loss: 0.3733 Avg Valid Loss: 0.4419 / \n",
      "Epoch 80 Avg Train Loss: 0.3782 Avg Valid Loss: 0.4631 / \n",
      "Epoch 81 Avg Train Loss: 0.3776 Avg Valid Loss: 0.4785 / \n",
      "Epoch 82 Avg Train Loss: 0.3787 Avg Valid Loss: 0.4385 / \n",
      "Epoch 83 Avg Train Loss: 0.3774 Avg Valid Loss: 0.4362 / \n",
      "Epoch 84 Avg Train Loss: 0.3747 Avg Valid Loss: 0.4550 / \n",
      "Epoch 85 Avg Train Loss: 0.3739 Avg Valid Loss: 0.4385 / \n",
      "Epoch 86 Avg Train Loss: 0.3742 Avg Valid Loss: 0.4530 / \n",
      "Epoch 87 Avg Train Loss: 0.3629 Avg Valid Loss: 0.4527 / \n",
      "Epoch 88 Avg Train Loss: 0.3763 Avg Valid Loss: 0.4489 / \n",
      "Epoch 89 Avg Train Loss: 0.3706 Avg Valid Loss: 0.4677 / \n",
      "Epoch 90 Avg Train Loss: 0.3746 Avg Valid Loss: 0.4839 / \n",
      "Epoch 91 Avg Train Loss: 0.3738 Avg Valid Loss: 0.4512 / \n",
      "Epoch 92 Avg Train Loss: 0.3763 Avg Valid Loss: 0.4330 / \n",
      "Epoch 93 Avg Train Loss: 0.3756 Avg Valid Loss: 0.4290 / \n",
      "Epoch 94 Avg Train Loss: 0.3652 Avg Valid Loss: 0.4711 / \n",
      "Epoch 95 Avg Train Loss: 0.3711 Avg Valid Loss: 0.4788 / \n",
      "Epoch 96 Avg Train Loss: 0.3722 Avg Valid Loss: 0.4343 / \n",
      "Epoch 97 Avg Train Loss: 0.3692 Avg Valid Loss: 0.4406 / \n",
      "Epoch 98 Avg Train Loss: 0.3718 Avg Valid Loss: 0.4564 / \n",
      "Epoch 99 Avg Train Loss: 0.3752 Avg Valid Loss: 0.4589 / \n",
      "Epoch 100 Avg Train Loss: 0.3752 Avg Valid Loss: 0.4431 / \n",
      "========== fold: 4 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.4226\n",
      "============ CV score with best loss weights ============\n",
      "Stage 0: 0.9023\n",
      "============ CV score with best loss weights ============\n",
      "Stage 1: 0.4076\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" and CFG.train_by_folds:\n",
    "    seed_torch(seed=CFG.seed)\n",
    "\n",
    "    stages_scores = {i: [] for i in CFG.train_stages}\n",
    "    stages_oof_df = {i: pd.DataFrame() for i in CFG.train_stages}\n",
    "\n",
    "    for fold in CFG.train_folds:\n",
    "\n",
    "        prev_dir = \"\"\n",
    "        for stage in range(len(CFG.total_evaluators)):\n",
    "\n",
    "            pop_dir = f\"{OUTPUT_DIR}pop_{stage+1}_weight_oof/\"\n",
    "            if not os.path.exists(pop_dir):\n",
    "                os.makedirs(pop_dir)\n",
    "\n",
    "            if stage not in CFG.train_stages:\n",
    "                prev_dir = pop_dir\n",
    "                continue\n",
    "\n",
    "            train_oof_df, score = train_loop(\n",
    "                stage=stage + 1,\n",
    "                epochs=CFG.epochs[stage],\n",
    "                fold=fold,\n",
    "                folds=train_pops[stage],\n",
    "                directory=pop_dir,\n",
    "                prev_dir=prev_dir,\n",
    "                eggs=all_eegs,\n",
    "            )\n",
    "\n",
    "            stages_oof_df[stage] = pd.concat([stages_oof_df[stage], train_oof_df])\n",
    "            stages_scores[stage].append(score)\n",
    "\n",
    "            prev_dir = pop_dir\n",
    "\n",
    "            LOGGER.info(f\"========== fold: {fold} stage: {stage+1} result ==========\")\n",
    "            LOGGER.info(f\"Score with best loss weights stage{stage+1}: {score:.4f}\")\n",
    "\n",
    "    for stage, scores in stages_scores.items():\n",
    "        LOGGER.info(f\"============ CV score with best loss weights ============\")\n",
    "        LOGGER.info(f\"Stage {stage}: {np.mean(scores):.4f}\")\n",
    "\n",
    "    for stage, oof_df in stages_oof_df.items():\n",
    "        pop_dir = f\"{OUTPUT_DIR}pop_{stage+1}_weight_oof/\"\n",
    "        oof_df.reset_index(drop=True, inplace=True)\n",
    "        oof_df.to_csv(\n",
    "            f\"{pop_dir}{CFG.model_name}_oof_df_ver-{CFG.VERSION}_stage-{stage+1}.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "    if CFG.wandb:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a221005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
